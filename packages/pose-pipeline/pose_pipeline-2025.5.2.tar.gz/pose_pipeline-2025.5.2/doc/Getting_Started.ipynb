{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PosePipeline : Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Description\n",
    "The following notebook will help guide and explain PosePipeline which is a pose estimation framework that uses computer vision to detect and track the posture of a person in an image or video. It is assumed that the input for the video was from a single camera (monocular). When working with pose estimation you can approach this with either top-down or bottom-up techniques.\n",
    "Top-down, first detects and locates all the people in the image (Tracking via bounding box), then extracts the pose (key points) for the person of interest. Bottom-up consists of locating all the key points first then associating them with the person of interest. A downside of top-down approaches is that the pose estimation depends a lot on the performance of the detection model. In a crowded scene, when person detection fails, pose estimation will fail, so perhaps top-down might not be as well suited for crowded multi-person scenes. \n",
    "\n",
    "This tutorial notebook will walk through some basic DataJoint operations, video import, and will process videos to obtain 3D keypoints for the person you are analyzing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "from IPython.display import Video as JupyterVideo\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "from os import system, name\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'        # Set the GPU to use for computation \n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'      # Set the OpenGL platform to use for rendering. omesa is a backend for OpenGL that doesn't require a display. This is useful for running on a server without a display. \n",
    "\n",
    "import datajoint as dj\n",
    "dj.config['display.limit'] = 50                 # Sets the limit for the number of rows to display when running a query, adjust as needed\n",
    "\n",
    "from pose_pipeline import *\n",
    "from pose_pipeline.utils.jupyter import play, play_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Introduction to DataJoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand DataJoint Description</summary>\n",
    "DataJoint is an open-source data mangement framework designed for scientific workflows. It helps researchers organize, process, and query complex data pipelines using Python (or MATLAB) with a clean, relational database backend like MySQL or PostgreSQL.\n",
    "\n",
    "It is highly reccommneded to look over the DataJoint Documentation: https://datajoint.com/docs/\n",
    "\n",
    "At its core, DataJoint: Structures your data into tables called schemas (relations) that represent experiments, results, or processing steps. Tracks dependencies between steps in your pipeline. Makes it easy to reproducibly populate, update, and query data — especially when dealing with many subjects, sessions, or models. Instead of manually managing files and folders, DataJoint schemas (a set of linked tables) to ensure data integrity, transparency, and scalability.\n",
    "\n",
    "DataJoint is a relational database framework and in PosePipe organizes data into three key table (schemas) types. \n",
    "(1)\tLookup Tables: Stores predefined, static values used for standardization\n",
    "(2)\tManual Tables: Contain user-inserted data that links method or experiment details. \n",
    "(3)\tComputed Tables: ‘Automatically’ generate results by processing upstream data. These tables rely on the populate() command to execute and store results. \n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.  List All Schemas in PosePipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = dj.list_schemas()\n",
    "print('The comprehensive list of all the Schemas in the database:')\n",
    "display(schemas)\n",
    "\n",
    "\n",
    "schema = dj.schema('pose_pipeline')\n",
    "print('The tables in the PosePipeline Schema include:')\n",
    "display(schema.list_tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. View the Schemas as Diagram\n",
    "Diagrams are a great way to visualize the pipeline and understand the flow of data. Diagrams witin DataJoint are based on entitiy relationship diagrams (ERD). \n",
    "Each node is a table. Arrows show dependencies between tables (e.g., foreign key relationships). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the ERD diagram\n",
    "# Recall schema = dj.Schema('pose_pipeline')\n",
    "\n",
    "diagram = dj.Diagram(schema)\n",
    "\n",
    "diagram.label = 'plain'                                             # Sets the label to plain for a simpler diagram\n",
    "\n",
    "filename = 'pose_pipeline_schema_diagram.png'                       # Sets the filename for the diagram image\n",
    "dj.Diagram(schema).save(filename)                                   # saves PNG image file of diagram in your current working directory\n",
    "Image(filename)                                                     # Displays the image of the diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.  Viewing a table definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The table definition for the Video is:\\n')\n",
    "print(Video.describe())\n",
    "print()\n",
    "\n",
    "print('The table definition for the TopDownMethodLookup is:\\n')\n",
    "print(TopDownMethodLookup.describe())\n",
    "print()\n",
    "\n",
    "print('The table definition for the TopDownPerson is:\\n')\n",
    "print(TopDownPerson.describe())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Output Explanation</summary>\n",
    "Viewing a schema's/ table's definition will tell you what data is in the table. If working in VS code you can also <Ctrl + click> on Video in the code cell and it will take you to the tables definition block. \n",
    "\n",
    "Breaking down the output:\n",
    "- Hashtag (#) followed by string is the description of the table\n",
    "- Three consecutive dashes (---) seperates primary keys (above the dashes) from depedent attributes (after the dashes)\n",
    "- Dash with greater than sign (->) indicates a foreign key dependency on another table - meaning that TopDownPerson inherits the primary keys of TopDownMethod\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. View a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['display.limit'] = 5\n",
    "\n",
    "display(VideoInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Querying and Filtering Data from a Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "DataJoint uses 'fetch' to query or pull data from tables. Below are some examples using this built in keyword. \n",
    "The '&' symbol in DataJoint is used to perform a restriction - like applying a filter to a table or query\n",
    "- (example 1) When fetch is used with the word KEY (has to all be capitalized) it pulls all the primary key fields from the table you specify \n",
    "- (example 2) How to fetch unique fields from a table as a numpy array\n",
    "- (example 3) How to filter a table by a specific attribute, in this case by project\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (example 1)\n",
    "primary_keys = Video.fetch('KEY')\n",
    "print('The primary keys for the Video table are:')\n",
    "display(primary_keys[1])                                                # Displays only the first primary key in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (example 2)\n",
    "np.unique(Video.fetch('video_project'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (example 3)\n",
    "proj_filt = {'video_project': 'gymnastics_TEST'}\n",
    "\n",
    "Video & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  PosePipeline: Raw Video Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "Assumes monocular video input. The first step in the process is to upload and import your videos. PosePipeline relies on DataJoint (SQL database) to store and process these videos. \n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import Video(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.utils.video_format import insert_local_video\n",
    "\n",
    "videos_path = '/mnt/CottonLab/datasets/gymnastics/'      # Path to the videos, adjust as needed\n",
    "files = os.listdir(videos_path)                          # List of all the files in the directory\n",
    "\n",
    "for f in files: \n",
    "    insert_local_video(f, datetime.now(), os.path.join(videos_path, f), video_project='gymnastics_TEST', skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Check that the Videos Table has been filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_filt = {'video_project': 'gymnastics_TEST'}\n",
    "Video & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Populate the VideoInfo Table\n",
    "Populate is a keyword in datajoint that fills downstream tables. In this case VideoInfo is a computed table that inherits from the Video table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VideoInfo.populate(proj_filt)\n",
    "VideoInfo & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. View a raw video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JupyterVideo('/mnt/CottonLab/datasets/gymnastics/' + 'gymnastics_test_1.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PosePipeline: Bottom-Up Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Click to Expand Explanation</summary>\n",
    "After you have uploaded your videos, a typical processing step is to select and run a bottom-up approach. Recall that Bottom-Up first finds all the keypoints in the image then associates them with a bounding box.  The following takes video(s) from the Video table and links them with a chosen bottom-up approach (e.g., OpenPose, MMPose, Bridging_OpenPose). BottomupPeople runs pose estimation by extracting and storing 2D keypoints (pixel locations). Optionally you can populate BottomUpVideo which takes the keypoints from BottomUpPeople and overlays them onto the video. \n",
    "\n",
    "To produce most overlay videos in this framework, you must first populate the BlurredVideo Table which protects human subjects' identities. BlurredVideo is dependent on first having keypoints for the face to know where to apply the blur to so BottomUpPeople must be populated. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.  Creating Keys to Process Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation of 3.1.</summary>\n",
    "To run (populate) BottomUpPeople you will need to specify the bottom-up method that you want to use. To acomplish this you will need to insert the proper rows into the BottomUpMethod table (Mannual Table). \n",
    "To do this you need to specify the primary keys (PK) for that table. Below is an example of how you may do this. First becuase BottomUpMethod inherits the PKs from Video which are 'video_project' and 'filename' we can just 'grab' those keys by using the special keyword 'KEY'. Then from here you can simply insert the name of the method you want to run. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''Grab' PKs from the Video table given by the 'KEY' in all caps. \n",
    "video_keys = (Video & proj_filt).fetch('KEY')\n",
    "display(video_keys)\n",
    "\n",
    "for v in video_keys:\n",
    "    v[\"bottom_up_method_name\"] = \"Bridging_OpenPose\"        # Set the method name that you want to use\n",
    "    print(v)\n",
    "    BottomUpMethod.insert1(v, skip_duplicates=True)\n",
    "\n",
    "BottomUpMethod() & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.  Populate BottomUpPeople Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: Populate BottomUpPeople\n",
    "BottomUpPeople.populate(proj_filt)      # Runs BottomUpPeople for the specified filter\n",
    "\n",
    "BottomUpPeople() & proj_filt            # Displays the populated BottomUpPeople table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand for Additional Info </summary>\n",
    "BottomUpPeople will run wihout having to define what is referred to here as a 'filter_skeleton'. BottomUpPeople produces a list of 2D keypoints and for the method of Bridging_OpenPose this specific method can optionally take in a 'filter_skeleton'. This is a skeleton that the user chooses and is responsible for selecting certain keypoints from the full list of output keypoints that belong to a specific skeleton format. Different datasets use different joint conventions (e.g., COCO, SMPL, etc). The other place where the skelton is directly used is in the BottomUpBridgingVideoLookup Table in which you must specify the skeleton to visualize the video. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_keys = []  # Create an empty list to store modified dictionaries\n",
    "\n",
    "for sk in video_keys:\n",
    "    sk_copy = sk.copy()  # Make a copy to avoid modifying the original\n",
    "    sk_copy[\"skeleton\"] = \"bml_movi_87\"\n",
    "    skeleton_keys.append(sk_copy)  # Append to the list\n",
    "\n",
    "display(skeleton_keys) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.  Populate BlurredVideo Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlurredVideo.populate(proj_filt)\n",
    "\n",
    "BlurredVideo() & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.  Populate BottomUpBridging Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Click to Expand Explanation</summary>\n",
    "BottomUpBridging is a type of bottom-up method that follows a slightly different workflow than other bottom-up methods because it ‘bridges’ (integrates) pose estimation with tracking and additional refinement. Unlike BottomUpPeople, which just extracts keypoints per frame, BottomUpBridging links these keypoints over time to (1) assign consistent IDs to people across frames, (2) estimates 3D keypoints instead of just 2D, and (3) tracks movement more robustly. \n",
    "If you want to see more granularly how this works go into bridging.py. The bridging_formats_bottom_up function in this sub-module processes each frame of the video and extracts bounding boxes, 2D keypoints, 3D keypoints, and the keypoint noise using the MeTRAbs model. The MeTRAbs (Metric-Scale Trained Regression for Absolute 3D Human Pose Estimation) is a deep learning-based model. A key challenge is that different frames may detect new people or lose track of previously detected people. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.pipeline import BottomUpBridging,BottomUpBridgingPerson\n",
    "BottomUpBridging.populate(proj_filt)    \n",
    "BottomUpBridging & proj_filt            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.  Indexing and Extracting Data From Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (BottomUpBridging & proj_filt).fetch('boxes', 'keypoints2d', 'keypoints3d')\n",
    "\n",
    "boxes, keypoints_2d, keypoints_3d = data\n",
    "\n",
    "print( np.array(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tbox = boxes[0][0]        # Extract first detected bounding box in the first frame\n",
    "# The bounding box is represented as [x1, y1, x2, y2, confidence_score]\n",
    "print(first_tbox)\n",
    "\n",
    "confidence_score = first_tbox[0,4]          # Confidence score of the first detected bounding box\n",
    "print(\"Confidence Score:\", confidence_score)\n",
    "\n",
    "x_min,y_min,width,height,confidence = first_tbox[0]\n",
    "print(x_min,y_min,width,height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.  Visualize the Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the data - Manually \n",
    "\n",
    "# Read the first frame of the video\n",
    "cap = cv2.VideoCapture('/mnt/CottonLab/datasets/gymnastics/' + 'gymnastics_test_1.mp4')  # Replace with the actual path\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "# Convert frame to RGB\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x_min, y_min, width, height = int(x_min), int(y_min), int(width), int(height)\n",
    "cv2.rectangle(frame_rgb, (x_min, y_min), (x_min + width, y_min + height), (0, 255, 0), 2)\n",
    "\n",
    "# Show the image with bounding boxes\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Bounding Box on Frame 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the data - Automatically\n",
    "from pose_pipeline.pipeline import BottomUpBridgingVideo\n",
    "BottomUpBridgingVideo.populate(skeleton_keys)\n",
    "BottomUpBridgingVideo() & proj_filt\n",
    "BottomUpBridgingVideo() & skeleton_keys\n",
    "\n",
    "video_path = (BottomUpBridgingVideo() & skeleton_keys).fetch(\"output_video\")\n",
    "print(video_path)\n",
    "\n",
    "# Display the video in Jupyter Notebook\n",
    "# Uncomment the following lines to display the video in Jupyter Notebook\n",
    "# from IPython.display import Video as JupyterVideo\n",
    "# JupyterVideo(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation </summary>\n",
    "Up until this point you have likely run or populated BottomUpBriding and or  BottomUpPeople. These methods have now provided you with boxes, 2D and 3D keypoints and 2D keypoints, respectively for all the people 'seen' in your videos. If your videos contain multiple people it is likely that you are interested in one particular subject. The following steps function to isolate this subject of interest for further analysis. To acomplish this you will need to run Tracking. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.pipeline import TrackingBbox, TrackingBboxMethod, TrackingBboxMethodLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Create Tracking Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the tracking method you want to use via TrackingBboxMethod table (this is a mannual table)\n",
    "\n",
    "tracking_method = (TrackingBboxMethodLookup & 'tracking_method_name=\"MMDet_deepsort\"').fetch1('tracking_method')\n",
    "print('The tracking method is: ', tracking_method)\n",
    "\n",
    "tracking_keys = (Video & proj_filt).fetch('KEY')\n",
    "display(tracking_keys)\n",
    "\n",
    "for key in tracking_keys:\n",
    "    key[\"tracking_method\"] = tracking_method\n",
    "    print(key)\n",
    "    TrackingBboxMethod.insert1(key, skip_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackingBboxMethod() & proj_filt\n",
    "TrackingBboxMethodLookup & 'tracking_method=8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.  Run Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackingBbox.populate(tracking_keys)\n",
    "\n",
    "TrackingBbox() & proj_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the video with the tracks overlaid by populating the TrackingBboxVideo Table (BlurredVideo has to be populated first)\n",
    "TrackingBboxVideo.populate(proj_filt)\n",
    "\n",
    "TrackingBboxVideo & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.  Working with the Annotations GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "After populating the TrackingBbox table, if dealing with a multi-person scene and or if you have detections that are not relevant you will need to select the tracking bounding boxes that contain the person that you are intereseted in analyzing. To do this, you will first visually identify the person you wnat and select ALL the tracks that locate them throughout the video. For internal users: follow the link below to access the annotation GUI.\n",
    "\n",
    "http://jc-compute01.ric.org:8505/\n",
    "\n",
    "The selected tracks are automatically populated into the PerpsonBboxValid table.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.pipeline import PersonBboxValid\n",
    "# Populated from annotations GUI\n",
    "PersonBboxValid & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.  Extract and view the valid tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = (PersonBboxValid & proj_filt).fetch('keep_tracks')\n",
    "display(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Populate PersonBbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "PersonBboxValid contains all the selected tracks/boxes that correspond to your person of interest. Next, you will populate PersonBbox which combines all the valid bboxes into a single bbox for the entire video. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.pipeline import PersonBbox\n",
    "PersonBbox.populate(proj_filt)\n",
    "PersonBbox & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.6. Assess the quality of the tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "Up to this point, you have tracked all people in the scene, selected which bounding boxes are valid or in other words the bounding box(es) that are associated with the subject of interest. DetectedFrames is an optional table that provides insight on the tracking quality. \n",
    "Depending on the method you want you can choose top-down or bottom-up. Because we already did part of BotomUpBridging you could populate BottomUpBridgingPerson which essentially filters out the other people in the scene and associates bounding boxes with motion keypoints. On the other hand you can choose to continue with a different top-down approach. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.pipeline import DetectedFrames\n",
    "\n",
    "DetectedFrames.populate(proj_filt)\n",
    "DetectedFrames & proj_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BottomUpBridgingPerson.populate(proj_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Top-Down Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "You may want to run Top-Down approaches as well. This could be because you want to test out some different tracking methods if the tracking performed in BottomUpBridging (OpenPose_Bridging) was not super successful. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.  Create Top-Down Method key(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_down_keys = (PersonBbox & proj_filt).fetch('KEY')\n",
    "display(top_down_keys)\n",
    "\n",
    "for td in top_down_keys:\n",
    "    td[\"top_down_method\"] = top_down_method\n",
    "    TopDownMethod.insert1(td, skip_duplicates=True)\n",
    "\n",
    "display(TopDownMethod() & proj_filt)\n",
    "\n",
    "top_down_keys = (TopDownMethod & proj_filt).fetch('KEY')\n",
    "display(top_down_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Populate the Top-Down Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopDownPerson.populate(proj_filt)\n",
    "TopDownPerson() & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Explanation</summary>\n",
    "The goal of lifting is to take in your 2D keypoints and produce them in 3D. To run lifting you’ll follow the same general principles as previous steps. First you’ll select your lifting method. For this example I will use the ‘Bridging_method_name = “Bridging_bml_movi_87”. You’ll create the key(s) for this specific method by inheriting the keys from TopDownPerson then add your lifting method to the key(s). From here you can check that you have defined all your PKs and populate LiftingPerson. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.  Create lifting keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting_keys = (TopDownPerson & proj_filt).fetch('KEY')\n",
    "display(lifting_keys)\n",
    "\n",
    "for L in lifting_keys:\n",
    "    L[\"lifting_method\"] = 12\n",
    "    LiftingMethod.insert1(L, skip_duplicates=True)\n",
    "\n",
    "display(LiftingMethod() & proj_filt)\n",
    "\n",
    "lifting_keys = (LiftingMethod & proj_filt).fetch('KEY')\n",
    "display(top_down_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.  Run Lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiftingPerson.populate(proj_filt)\n",
    "LiftingPerson() & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Lifting Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiftingPersonVideo.populate(proj_filt)\n",
    "LiftingPersonVideo() & proj_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to Expand Final Summary</summary>\n",
    " You have now reached the end of the tutorial notebook for PosePipeline. You began with raw images or videos that you wanted to analyze. If done correctly, you should be left with 2D and 3D keypoints that have been associated to the person you are interested in analyzing, along with the keypoint confidences. If you are familiar with marker-based motion capture this is similar to reaching the end of data collection where you are left with the 3D marker positions in space. The keypoints that you have. \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
