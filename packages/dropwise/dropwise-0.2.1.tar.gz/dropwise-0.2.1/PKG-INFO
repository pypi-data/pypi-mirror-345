Metadata-Version: 2.4
Name: dropwise
Version: 0.2.1
Summary: Monte Carlo Dropout-based uncertainty estimation for Transformers
Home-page: https://github.com/aryanator/dropwise
Author: Aryan Patil
Author-email: aryanator01@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENCE
Requires-Dist: torch
Requires-Dist: transformers
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Dropwise

**Dropwise** is a lightweight PyTorch/HuggingFace wrapper for performing Monte Carlo Dropoutâ€“based uncertainty estimation in Transformers. It enables confidence-aware decision making by revealing how certain a model is about its predictions â€” all with just a few lines of code.

---

## ğŸš€ Features

- âœ… Enable dropout during inference for **Bayesian-like uncertainty** estimation  
- âœ… Compute **predictive entropy**, **confidence**, and **per-class standard deviation**  
- âœ… Modular support for **classification, QA, token tagging, and regression**  
- âœ… Works seamlessly with **Hugging Face Transformers** and **PyTorch**  
- âœ… Supports **batch inference**, **CPU/GPU**, and customizable `num_passes`  
- âœ… Cleanly packaged and extensible for research or production

---

## ğŸ¤– Supported Tasks

| Task Type               | Example Model                                 |
|------------------------|------------------------------------------------|
| `sequence-classification` | `distilbert-base-uncased-finetuned-sst-2-english`  
| `token-classification`    | `dslim/bert-base-NER`  
| `question-answering`      | `deepset/bert-base-cased-squad2`  
| `regression`              | `roberta-base` (with custom regression head)

> âš ï¸ Your model must contain dropout layers for MC sampling to work (most HF models do).

---

## ğŸ“¦ Installation

```bash
pip install dropwise
```

Or install from source:

```bash
git clone https://github.com/aryanator01/dropwise.git
cd dropwise
pip install -e .
```

---

## ğŸ§  Example Usage (Per Task)

### ğŸ“˜ Sequence Classification

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from dropwise.predictor import DropwisePredictor

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

predictor = DropwisePredictor(model, tokenizer, task_type="sequence-classification", num_passes=20)
results = predictor(["The movie was fantastic!"])

print(results[0])
```

---

### ğŸ·ï¸ Token Classification (NER)

```python
from transformers import AutoModelForTokenClassification, AutoTokenizer
from dropwise.predictor import DropwisePredictor

model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")

predictor = DropwisePredictor(model, tokenizer, task_type="token-classification", num_passes=15)
results = predictor(["Hugging Face is based in New York City."])

print(results[0]['token_predictions'])
```

---

### â“ Question Answering

```python
from transformers import AutoModelForQuestionAnswering, AutoTokenizer
from dropwise.predictor import DropwisePredictor

model = AutoModelForQuestionAnswering.from_pretrained("deepset/bert-base-cased-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/bert-base-cased-squad2")

question = "Where is Hugging Face based?"
context = "Hugging Face Inc. is a company based in New York City."
qa_input = f"{question} [SEP] {context}"

predictor = DropwisePredictor(model, tokenizer, task_type="question-answering", num_passes=10)
results = predictor([qa_input])

print(results[0]['answer'])
```

---

### ğŸ“ˆ Regression

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from dropwise.predictor import DropwisePredictor

model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)
tokenizer = AutoTokenizer.from_pretrained("roberta-base")

predictor = DropwisePredictor(model, tokenizer, task_type="regression", num_passes=20)
results = predictor(["The child is very young."])

print(results[0]['predicted_score'], "+/-", results[0]['uncertainty'])
```

---

## ğŸ“Š Output Dictionary (per sample)

| Field               | Description                                      |
|--------------------|--------------------------------------------------|
| `predicted_class`  | Index of most probable class (classification)    |
| `predicted_score`  | Scalar prediction (regression only)              |
| `confidence`       | Highest softmax probability                      |
| `entropy`          | Predictive entropy (higher = less confident)     |
| `std_dev`          | Standard deviation across MC passes              |
| `probs`            | Class-wise softmax probabilities                 |
| `margin`           | Confidence gap between top-2 predictions         |
| `answer`           | Predicted answer span (QA only)                  |
| `token_predictions`| List of per-token dicts (token classification)   |

---

## ğŸ§ª Run Tests

```bash
python tests/test_predictor.py
```

Covers all task types with preloaded models.

---

## ğŸ“‚ Folder Structure

```
dropwise/
â”œâ”€â”€ predictor.py
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sequence_classification.py
â”‚   â”œâ”€â”€ token_classification.py
â”‚   â”œâ”€â”€ question_answering.py
â”‚   â””â”€â”€ regression.py
tests/
â””â”€â”€ test_predictor.py
```

---

## ğŸ“ License

MIT License

---

Built with â¤ï¸ for robust, explainable, uncertainty-aware AI systems.
