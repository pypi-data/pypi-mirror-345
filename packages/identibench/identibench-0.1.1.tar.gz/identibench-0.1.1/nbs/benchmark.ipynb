{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from collections.abc import Iterator, Callable\n",
    "from typing import Any\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from identibench.utils import get_default_data_root,_load_sequences_from_files, hdf_files_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions\n",
    "import identibench.metrics\n",
    "from identibench.utils import _dummy_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def aggregate_metric_score(test_results,metric_func, score_name=None ,sequence_aggregation_func=np.mean,window_aggregation_func=np.mean):\n",
    "    # iterate over test_results and calculate metric score for each (y_pred,y_test) tuple, if prediction, iterate over nested tuples with nested loop\n",
    "    if score_name is None:\n",
    "        score_name = metric_func.__name__\n",
    "    if isinstance(test_results[0], list):\n",
    "        scores = []\n",
    "        for windowed_sequence in test_results:\n",
    "            scores.append(window_aggregation_func([metric_func(y_pred,y_test) for y_pred,y_test in windowed_sequence]))\n",
    "    else:\n",
    "        scores = [[metric_func(y_pred,y_test) for y_pred,y_test in test_results]]\n",
    "    return {score_name: sequence_aggregation_func(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BenchmarkSpecBase: pass \n",
    "class BenchmarkSpecBase:\n",
    "    \"\"\"\n",
    "    Base class for benchmark specifications, holding common attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str, # Unique name identifying this benchmark task.\n",
    "                 dataset_id: str, # Identifier for the raw dataset source.\n",
    "                 u_cols: list[str], # list of column names for input signals (u).\n",
    "                 y_cols: list[str], # list of column names for output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                 x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "                 sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "                 download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "                 test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = None,\n",
    "                 custom_test_evaluation = None, \n",
    "                 init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                 data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_model_func = test_model_func\n",
    "        self.custom_test_evaluation = custom_test_evaluation\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        if isinstance(self._data_root, Callable):\n",
    "            return self._data_root()\n",
    "        return self._data_root\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "    \n",
    "    @property\n",
    "    def test_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the test files .\"\"\"\n",
    "        return self.dataset_path / 'test'\n",
    "    \n",
    "    @property\n",
    "    def train_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the train files.\"\"\"\n",
    "        return self.dataset_path / 'train'\n",
    "    \n",
    "    @property\n",
    "    def valid_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the valid files.\"\"\"\n",
    "        return self.dataset_path / 'valid'\n",
    "    \n",
    "    @property\n",
    "    def train_valid_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the train_valid files.\"\"\"\n",
    "        return self.dataset_path / 'train_valid'\n",
    "    \n",
    "    @property\n",
    "    def train_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the training directory.\"\"\"\n",
    "        return hdf_files_from_path(self.train_path)\n",
    "    \n",
    "    @property\n",
    "    def valid_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the validation directory.\"\"\"\n",
    "        return hdf_files_from_path(self.valid_path)\n",
    "    \n",
    "    @property\n",
    "    def train_valid_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the train_valid directory if it exists, otherwise returns the union of the train and valid directories.\"\"\"\n",
    "        train_valid_files = hdf_files_from_path(self.train_valid_path)\n",
    "        if train_valid_files: \n",
    "            return train_valid_files\n",
    "        else:\n",
    "            train_files = hdf_files_from_path(self.train_path)\n",
    "            valid_files = hdf_files_from_path(self.valid_path)\n",
    "            return sorted(train_files+valid_files)\n",
    "\n",
    "    @property\n",
    "    def test_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the test directory.\"\"\"\n",
    "        return hdf_files_from_path(self.test_path)\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"Checks if the dataset exists, downloads/prepares it if needed.\"\"\"\n",
    "        # (Implementation remains the same as before)\n",
    "        dataset_path = self.dataset_path\n",
    "        if self.download_func is None:\n",
    "            print(f\"Warning: No download function for '{self.name}'. Assuming data exists at {dataset_path}\")\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        if not dataset_path.is_dir() or force_download:\n",
    "            print(f\"Preparing dataset for '{self.name}' at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                self.download_func(dataset_path, force_download)\n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export    \n",
    "def _test_simulation(specs, model):\n",
    "    results = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(specs.test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        y_test_win = y_test[specs.init_window:]\n",
    "        y_pred = y_pred[-y_test_win.shape[0]:]\n",
    "        results.append((y_pred,y_test_win))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecSimulation(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a simulation benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec.\n",
    "    Use this when the goal is to simulate the system's output given the input `u`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                name: str, # Unique name identifying this benchmark task.\n",
    "                dataset_id: str, # Identifier for the raw dataset source.\n",
    "                u_cols: list[str], # list of column names for input signals (u).\n",
    "                y_cols: list[str], # list of column names for output signals (y).\n",
    "                metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "                sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "                download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "                test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = _test_simulation,\n",
    "                custom_test_evaluation = None, \n",
    "                init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "            ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_model_func = test_model_func\n",
    "        self.custom_test_evaluation = custom_test_evaluation\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti    \n",
    "def _test_prediction(specs, model):\n",
    "    results = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(specs.test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        #iterate through windows of u_test and y_test\n",
    "        window_results = []\n",
    "        for i in range(0, u_test.shape[0] - specs.init_window, specs.pred_step):\n",
    "            u_test_win = u_test[i:i+specs.init_window]\n",
    "            y_test_win = y_test[i:i+specs.init_window]\n",
    "            y_pred = model(u_test_win,y_test_win[:specs.init_window])\n",
    "            window_results.append((y_pred,y_test_win))\n",
    "        results.append(window_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecPrediction(BenchmarkSpecBase):\n",
    "     \"\"\"\n",
    "     Specification for a k-step ahead prediction benchmark task.\n",
    "\n",
    "     Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
    "     Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.\n",
    "     \"\"\"\n",
    "     def __init__(self,\n",
    "               name: str, # Unique name identifying this benchmark task.\n",
    "               dataset_id: str, # Identifier for the raw dataset source.\n",
    "               u_cols: list[str], # list of column names for input signals (u).\n",
    "               y_cols: list[str], # list of column names for output signals (y).\n",
    "               metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "               pred_horizon: int, # The 'k' in k-step ahead prediction (mandatory for this type).\n",
    "               pred_step: int, # Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n",
    "               x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "               sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "               download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "               test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = _test_prediction,\n",
    "               custom_test_evaluation = None, \n",
    "               init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "               data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "          ):\n",
    "          if pred_horizon <= 0:\n",
    "               raise ValueError(\"pred_horizon must be a positive integer for PredictionBenchmarkSpec.\")\n",
    "          self.pred_horizon = pred_horizon\n",
    "          self.pred_step = pred_step\n",
    "          \n",
    "          self.name = name\n",
    "          self.dataset_id = dataset_id\n",
    "          self.u_cols = u_cols\n",
    "          self.y_cols = y_cols\n",
    "          self.metric_func = metric_func\n",
    "          self.x_cols = x_cols\n",
    "          self.sampling_time = sampling_time\n",
    "          self.download_func = download_func\n",
    "          self.test_model_func = test_model_func\n",
    "          self.custom_test_evaluation = custom_test_evaluation\n",
    "          self.init_window = init_window\n",
    "          self._data_root = data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_sim = BenchmarkSpecSimulation(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "test_eq(_spec_sim.init_window, None)\n",
    "test_eq(_spec_sim.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred = BenchmarkSpecPrediction(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2\n",
    ")\n",
    "test_eq(_spec_pred.init_window, 20)\n",
    "test_eq(_spec_pred.pred_horizon, 5)\n",
    "test_eq(_spec_pred.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpecSimulation(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_ensure' at /Users/daniel/.identibench_data/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpecBase, # The benchmark specification.\n",
    "                 hyperparameters: dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: int|None = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.train_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.valid_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.train_valid_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(spec, build_model, hyperparameters={}, seed=None):\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "        'benchmark_type' : type(spec).__name__,\n",
    "        'metric_name': spec.metric_func.__name__,\n",
    "        'metric_score': np.nan,\n",
    "        'custom_scores': {},\n",
    "        'model_predictions': []\n",
    "    }\n",
    "\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=seed) \n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    test_start_time = time.monotonic()\n",
    "    test_results = spec.test_model_func(spec, model) # get list of (y_pred,y_test) tuples\n",
    "    test_end_time = time.monotonic()\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "\n",
    "    results['model_predictions'] = test_results\n",
    "\n",
    "    results.update(aggregate_metric_score(test_results, spec.metric_func, score_name='metric_score'))\n",
    "    if spec.custom_test_evaluation is not None:\n",
    "        results['custom_scores'] = spec.custom_test_evaluation(test_results, spec)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Define a very simple build_model function for the example\n",
    "def _dummy_build_model(context):\n",
    "    print(f\"Building model with spec: {context.spec.name}, seed: {context.seed}\")\n",
    "\n",
    "    def dummy_model(u_test,y_test):\n",
    "        output_dim = len(context.spec.y_cols) \n",
    "        return np.zeros((u_test.shape[0], output_dim))\n",
    "        \n",
    "    return dummy_model # Return the callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: _spec_default, seed: 1767533229\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "hyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n",
    "\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=_spec_sim, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': '_spec_default',\n",
       " 'dataset_id': '_dummy_default',\n",
       " 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n",
       " 'seed': 1767533229,\n",
       " 'training_time_seconds': 1.8167018424719572e-05,\n",
       " 'test_time_seconds': 0.001075083011528477,\n",
       " 'benchmark_type': 'BenchmarkSpecSimulation',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.5947432613358903,\n",
       " 'custom_scores': {}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: _spec_pred_params, seed: 627056196\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=_spec_pred, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': '_spec_pred_params',\n",
       " 'dataset_id': '_dummy_pred_params',\n",
       " 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n",
       " 'seed': 627056196,\n",
       " 'training_time_seconds': 5.262502236291766e-05,\n",
       " 'test_time_seconds': 0.0009175419982057065,\n",
       " 'benchmark_type': 'BenchmarkSpecPrediction',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.5808981003195128,\n",
       " 'custom_scores': {}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluation(results,spec):\n",
    "    def get_max_abs_error(y_pred,y_test):\n",
    "        return np.max(np.abs(y_test - y_pred))\n",
    "    def get_max_error(y_pred,y_test):\n",
    "        return np.max(y_test - y_pred)\n",
    "\n",
    "    avg_max_abs_error = aggregate_metric_score(results, get_max_abs_error, score_name='avg_max_abs_error',sequence_aggregation_func=np.mean,window_aggregation_func=np.mean)\n",
    "    median_max_error = aggregate_metric_score(results, get_max_error, score_name='median_max_abs_error',sequence_aggregation_func=np.median,window_aggregation_func=np.median)\n",
    "    return {**avg_max_abs_error, **median_max_error}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_with_custom_test = BenchmarkSpecSimulation(\n",
    "    name=\"CustomTestExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n",
    "    download_func=_dummy_dataset_loader, \n",
    "    u_cols=['u0', 'u1'], \n",
    "    y_cols=['y0'],\n",
    "    custom_test_evaluation=custom_evaluation,\n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: CustomTestExampleBench, seed: 86861679\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark using the spec with the custom test function\n",
    "hyperparams = {'model_type': 'dummy_v2'} \n",
    "\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=spec_with_custom_test, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': 'CustomTestExampleBench',\n",
       " 'dataset_id': 'dummy_core_data_v1',\n",
       " 'hyperparameters': {'model_type': 'dummy_v2'},\n",
       " 'seed': 86861679,\n",
       " 'training_time_seconds': 2.2249994799494743e-05,\n",
       " 'test_time_seconds': 0.0012531669926829636,\n",
       " 'benchmark_type': 'BenchmarkSpecSimulation',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.6100348166022335,\n",
       " 'custom_scores': {'avg_max_abs_error': 0.9871239066123962,\n",
       "  'median_max_abs_error': 0.9871239066123962}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_multiple_benchmarks(\n",
    "    specs: list[BenchmarkSpecBase] | dict[str, BenchmarkSpecBase], # Collection of specs to run\n",
    "    build_model: Callable[[TrainingContext], Callable], # User function to build the model/predictor\n",
    "    hyperparameters: dict[str, Any]|None = None, # Hyperparameters passed to build_model\n",
    "    seed: int|None = None, # Base random seed\n",
    "    continue_on_error: bool = True, # If True, continue running benchmarks even if one fails\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs multiple benchmarks sequentially using the same build_model function.\n",
    "\n",
    "    Args:\n",
    "        specs: A list or dictionary containing the BenchmarkSpec objects to run.\n",
    "        build_model: A callable that accepts a TrainingContext and returns a trained model/predictor function.\n",
    "        hyperparameters: A dictionary of hyperparameters passed to the build_model function.\n",
    "        seed: An optional integer seed passed to each run_benchmark call for reproducibility.\n",
    "        continue_on_error: If True, catches exceptions during individual benchmark runs, prints a warning,\n",
    "                           and continues. If False, stops on the first error.\n",
    "\n",
    "    Returns:\n",
    "        A list of result dictionaries containing the results from successful benchmark runs.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    hyperparameters = hyperparameters or {} # Ensure it's a dict\n",
    "\n",
    "    # Determine the list of specification objects to iterate over\n",
    "    spec_objects = list(specs.values()) if isinstance(specs, dict) else list(specs)\n",
    "\n",
    "    print(f\"--- Starting benchmark run for {len(spec_objects)} specifications ---\")\n",
    "\n",
    "    for i, spec in enumerate(spec_objects):\n",
    "        spec_name = getattr(spec, 'name', f'Unnamed Spec {i+1}') # Get name for logging\n",
    "        print(f\"\\n[{i+1}/{len(spec_objects)}] Running benchmark: {spec_name}\")\n",
    "\n",
    "        try:\n",
    "            # Run the individual benchmark\n",
    "            result = run_benchmark(\n",
    "                spec=spec,\n",
    "                build_model=build_model,\n",
    "                hyperparameters=hyperparameters,\n",
    "                seed=seed # Pass the same base seed to each run\n",
    "            )\n",
    "            results_list.append(result)\n",
    "            print(f\"  -> Success: {spec_name} completed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR running benchmark '{spec_name}': {e}\")\n",
    "            if not continue_on_error:\n",
    "                print(\"Stopping due to error (continue_on_error=False).\")\n",
    "                raise \n",
    "\n",
    "    print(f\"\\n--- Benchmark run finished. {len(results_list)}/{len(spec_objects)} completed successfully. ---\")\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting benchmark run for 3 specifications ---\n",
      "\n",
      "[1/3] Running benchmark: _spec_default\n",
      "Building model with spec: _spec_default, seed: 3125350751\n",
      "  -> Success: _spec_default completed.\n",
      "\n",
      "[2/3] Running benchmark: _spec_pred_params\n",
      "Building model with spec: _spec_pred_params, seed: 1263539863\n",
      "  -> Success: _spec_pred_params completed.\n",
      "\n",
      "[3/3] Running benchmark: CustomTestExampleBench\n",
      "Building model with spec: CustomTestExampleBench, seed: 3241224878\n",
      "  -> Success: CustomTestExampleBench completed.\n",
      "\n",
      "--- Benchmark run finished. 3/3 completed successfully. ---\n"
     ]
    }
   ],
   "source": [
    "benchmark_results = run_multiple_benchmarks(\n",
    "    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n",
    "    build_model=_dummy_build_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'benchmark_name': '_spec_default',\n",
       "  'dataset_id': '_dummy_default',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 3125350751,\n",
       "  'training_time_seconds': 1.1625001206994057e-05,\n",
       "  'test_time_seconds': 0.001053333020536229,\n",
       "  'benchmark_type': 'BenchmarkSpecSimulation',\n",
       "  'metric_name': 'rmse',\n",
       "  'metric_score': 0.5947432613358903,\n",
       "  'custom_scores': {}},\n",
       " {'benchmark_name': '_spec_pred_params',\n",
       "  'dataset_id': '_dummy_pred_params',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 1263539863,\n",
       "  'training_time_seconds': 3.4580007195472717e-06,\n",
       "  'test_time_seconds': 0.0003321249969303608,\n",
       "  'benchmark_type': 'BenchmarkSpecPrediction',\n",
       "  'metric_name': 'rmse',\n",
       "  'metric_score': 0.5808981003195128,\n",
       "  'custom_scores': {}},\n",
       " {'benchmark_name': 'CustomTestExampleBench',\n",
       "  'dataset_id': 'dummy_core_data_v1',\n",
       "  'hyperparameters': {},\n",
       "  'seed': 3241224878,\n",
       "  'training_time_seconds': 3.6250276025384665e-06,\n",
       "  'test_time_seconds': 0.0005491670162882656,\n",
       "  'benchmark_type': 'BenchmarkSpecSimulation',\n",
       "  'metric_name': 'rmse',\n",
       "  'metric_score': 0.6100348166022335,\n",
       "  'custom_scores': {'avg_max_abs_error': 0.9871239066123962,\n",
       "   'median_max_abs_error': 0.9871239066123962}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "for result in benchmark_results:\n",
    "    result.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
