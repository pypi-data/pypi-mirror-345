#@meta {desc: "Unsloth trainer configuration", date: "2025-02-11"}


## Resource
#
lmtask_trainer_unsloth_model:
  # unsloth auto supports RoPE Scaling internally
  max_seq_length: ${lmtask_trainer_default:max_seq_length}
  # none for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
  dtype: null
  # Use 4bit quantization to reduce memory usage. Can be False.
  load_in_4bit: True
  #
  # 4bit pre quantized models we support for 4x faster downloading + no OOMs.
  model_name: ${lmtask_trainer_default:source_model}


## Peft
#
lmtask_trainer_unsloth_peft:
  # choose any number > 0; suggested 8, 16, 32, 64, 128
  r: 64
  # add for continual pretraining
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens
    - lm_head
  lora_alpha: 32
  # supports any, but 0 is optimized
  lora_dropout: 0
  # supports any, but "none" is optimized
  bias: "none"
  # "unsloth" uses 30% less VRAM, fits 2x larger batch sizes
  # True or "unsloth" for very long context
  use_gradient_checkpointing: "unsloth"
  # random seed
  random_state: 0
  # we support rank stabilized LoRA
  use_rslora: true
  # and LoftQ
  loftq_config: null
  # temporary checkpoint data
  temporary_location: 'path: ${lmtask_trainer_default:checkpoint_dir}'

lmtask_trainer_unsloth_resource:
  class_name: zensols.lmtask.unsloth.UnslothTrainerResource
  model_args: 'asdict: lmtask_trainer_unsloth_model'
  peft_args: 'asdict: lmtask_trainer_unsloth_peft'


## Unsloth trainer
#
lmtask_trainer_unsloth_training_arguments:
  class_name: unsloth.UnslothTrainingArguments
  # the batch size per GPU/TPU core/CPU for training
  per_device_train_batch_size: 2
  # number of updates steps to accumulate the gradients for, before performing
  # a backward/update pass
  gradient_accumulation_steps: 8
  # ratio of total training steps used for a linear warmup from 0 to learning_rate
  warmup_ratio: 0.1
  # total number of training epochs to perform
  #num_train_epochs: 3
  # learning rate
  learning_rate: 5e-5
  # https://unsloth.ai/blog/contpretraining
  # We show we must use a smaller learning rate for the lm_head and
  # embed_tokens, and Unsloth handles this with our new UnslothTrainer and
  # UnslothTrainingArguments. Simply set embedding_learning_rate to be a
  # smaller number than the normal learning_rate. For example, 10x smaller or
  # 2x smaller
  embedding_learning_rate: 5e-6
  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit
  # training.
  fp16: "eval({'import': ['unsloth as u']}): not u.is_bfloat16_supported()"
  # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit
  # training. Requires Ampere or higher NVIDIA architecture or using CPU
  # (use_cpu) or Ascend NPU. This is an experimental API and it may change
  bf16: "eval({'import': ['unsloth as u']}): u.is_bfloat16_supported()"
  #logging_steps: 1
  optim: 'adamw_8bit'
  # the weight decay to apply (if not zero) to all layers except all bias and
  # LayerNorm weights in AdamW optimizer; defaults to 0
  #weight_decay: 0.00
  lr_scheduler_type: 'cosine'
  seed: 0
  output_dir: 'path: ${lmtask_trainer_default:checkpoint_dir}'
  # used for WandB
  report_to: 'none'

lmtask_trainer_unsloth_trainer_arguments:
  max_seq_length: ${lmtask_trainer_default:max_seq_length}
  dataset_num_proc: 8
  args: 'instance: lmtask_trainer_unsloth_training_arguments'

lmtask_trainer_unsloth:
  class_name: zensols.lmtask.unsloth.UnslothTrainer
  resource: 'instance: lmtask_trainer_unsloth_resource'
  trainer_params: 'asdict: lmtask_trainer_unsloth_trainer_arguments'
