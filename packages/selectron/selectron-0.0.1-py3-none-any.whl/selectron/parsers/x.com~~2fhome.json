{
  "python": "from bs4 import BeautifulSoup\nimport re\n\ndef parse_element(html: str) -> dict[str, str|int|list|dict]:\n    def parse_count(text: str) -> int | None:\n        if not text:\n            return None\n        text = text.strip().lower().replace(',', '')\n        multiplier = 1\n        if text.endswith('k'):\n            multiplier = 1000\n            text = text[:-1]\n        elif text.endswith('m'):\n            multiplier = 1000000\n            text = text[:-1]\n        try:\n            return int(float(text) * multiplier)\n        except Exception:\n            return None\n\n    result = {}\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Identify primary_url: link containing a <time datetime=...> child if possible\n    primary_url = None\n    datetime_val = None\n    time_elem = soup.find('time', datetime=True)\n    if time_elem and time_elem.parent.name == 'a':\n        primary_url = time_elem.parent.get('href')\n        datetime_val = time_elem.get('datetime')\n    if not primary_url:\n        # fallback: first <a> with href that looks like permalink\n        links = soup.find_all('a', href=True)\n        for link in links:\n            href = link['href']\n            if href and ('/status/' in href or '/post/' in href or '/item/' in href):\n                primary_url = href\n                break\n        if not primary_url and links:\n            primary_url = links[0]['href']\n\n    if primary_url:\n        result['primary_url'] = primary_url\n\n    if datetime_val:\n        result['datetime'] = datetime_val\n    else:\n        if time_elem:\n            datetime_val = time_elem.get('datetime')\n            if datetime_val:\n                result['datetime'] = datetime_val\n\n    # ID extraction - try from data-id attribute or last part of URL\n    id_val = None\n    if soup.has_attr('data-id'):\n        id_val = soup['data-id']\n    else:\n        if primary_url:\n            # extract last non-empty segment after /\n            parts = [p for p in primary_url.split('/') if p]\n            if parts:\n                id_val = parts[-1]\n    if id_val:\n        result['id'] = id_val\n\n    # Extract title: look in h1-h4, else aria-label on main container\n    title = None\n    for h_tag in ['h1', 'h2', 'h3', 'h4']:\n        h = soup.find(h_tag)\n        if h and h.get_text(strip=True):\n            title = h.get_text(strip=True)\n            break\n    if not title and soup.has_attr('aria-label'):\n        title = soup['aria-label']\n    if title:\n        result['title'] = title\n\n    # Author and author_url - find link near avatar or class\n    author = None\n    author_url = None\n    # Look for common author selectors\n    author_candidates = []\n    author_links = soup.select('a[href*=\"/profile\"], a[href*=\"/user\"], a[href*=\"/@\"], a.author, a[data-author]')\n    for al in author_links:\n        text = al.get_text(strip=True)\n        href = al.get('href')\n        if text and text.lower() not in ('verified', 'verified account'):\n            author_candidates.append((text, href))\n    if author_candidates:\n        author, author_url = author_candidates[0]\n    else:\n        # fallback: maybe textual @handle without link\n        text_candidates = soup.find_all(string=re.compile(r'@\\w+'))\n        if text_candidates:\n            author = text_candidates[0].strip()\n\n    if author:\n        result['author'] = author\n    if author_url:\n        result['author_url'] = author_url\n\n    # Description - find main content text excluding metadata\n    description = None\n    # Try to find a div or article with 'tweetText' id, attr, or class (common on social media content)\n    tweet_text_candidates = []\n    for attr in ['id', 'class', 'data-testid']:\n        val = soup.find(attrs={attr: re.compile(r'tweettext', re.I)})\n        if val and val.get_text(strip=True):\n            tweet_text_candidates.append(val.get_text(separator=' ', strip=True))\n    if tweet_text_candidates:\n        # pick longest\n        description = max(tweet_text_candidates, key=len)\n\n    if not description:\n        # fallback: largest div/article with >25 chars and excludes time, links with status\n        candidates = []\n        for tag in ['article', 'div', 'section']:\n            for elem in soup.find_all(tag):\n                if elem.find('time'):\n                    continue\n                links = elem.find_all('a', href=True)\n                if any('/status/' in a['href'] or '/post/' in a['href'] for a in links):\n                    continue\n                text = elem.get_text(separator=' ', strip=True)\n                if text and len(text) > 25:\n                    candidates.append((len(text), text))\n        if candidates:\n            description = max(candidates, key=lambda x: x[0])[1]\n\n    if description:\n        result['description'] = description\n\n    # Extract images\n    images = []\n    img_tags = soup.find_all('img')\n    for img in img_tags:\n        src = img.get('src', '')\n        if not src or src.startswith('data:'):\n            # exclude empty or data: uris\n            continue\n        # filter out author avatars by heuristics - size or class containing \"avatar\"\n        classes = img.get('class')\n        if classes and any('avatar' in c.lower() for c in classes):\n            continue\n        alt = img.get('alt')\n        title = img.get('title')\n        width = img.get('width')\n        height = img.get('height')\n        img_data = {'src': src}\n        if alt:\n            img_data['alt'] = alt\n        if title:\n            img_data['title'] = title\n        if width:\n            try:\n                img_data['width'] = int(width)\n            except Exception:\n                pass\n        if height:\n            try:\n                img_data['height'] = int(height)\n            except Exception:\n                pass\n        # include all data-* attrs on img\n        for attr, val in img.attrs.items():\n            if attr.startswith('data-'):\n                img_data[attr] = val\n        if img_data:\n            images.append(img_data)\n    if images:\n        result['images'] = images\n\n    # Extract metrics - look for numbers with labels reply, repost, like, bookmark, view\n    # Use class names and aria-labels\n    metrics_map = {\n        'reply_count': ['reply', 'replies', 'comments'],\n        'repost_count': ['repost', 'retweet', 'shares'],\n        'like_count': ['like', 'likes', 'favorite', 'favorites'],\n        'bookmark_count': ['bookmark', 'bookmarks'],\n        'view_count': ['view', 'views', 'impression', 'impressions'],\n    }\n\n    # regex to find numbers\n    number_re = re.compile(r'(\\d+[\\d,.]*)([kKmM]?)')\n\n    text_blocks = soup.find_all(text=True)\n    texts = [t.strip().lower() for t in text_blocks if len(t.strip()) <= 15 and re.search(r'\\d', t)]\n\n    counts_found = {}\n    for t in texts:\n        for key, keywords in metrics_map.items():\n            if any(kw in t for kw in keywords):\n                # extract number before keyword\n                for m in number_re.finditer(t):\n                    num_str, suffix = m.groups()\n                    cnt = parse_count(num_str + suffix)\n                    if cnt is not None:\n                        # store max if multiple found\n                        if key not in counts_found or cnt > counts_found[key]:\n                            counts_found[key] = cnt\n    if counts_found:\n        result.update(counts_found)\n\n    # rank from data-rank or aria-posinset\n    rank = None\n    if soup.has_attr('data-rank'):\n        try:\n            rank = int(soup['data-rank'])\n        except Exception:\n            pass\n    if not rank:\n        # fallback aria-posinset\n        rank_elems = soup.find_all(attrs={'aria-posinset': True})\n        for e in rank_elems:\n            try:\n                rank = int(e['aria-posinset'])\n                if rank:\n                    break\n            except Exception:\n                continue\n    if rank:\n        result['rank'] = rank\n\n    # Nested quoted content extraction\n    # Look for a blockquote or nested article/div with class containing \"quoted\"\n    quoted = None\n    quoted_elem = soup.find(['blockquote', 'article', 'div'], class_=re.compile(r'quoted', re.I))\n    if quoted_elem:\n        quoted = {}\n        # Extract quoted_author_name\n        quoted_author = None\n        author_link = quoted_elem.find('a', href=True)\n        if author_link:\n            quoted_author = author_link.get_text(strip=True)\n            quoted['quoted_author_name'] = quoted_author\n            quoted['quoted_url'] = author_link['href']\n        # Extract quoted_text\n        quoted_text = quoted_elem.get_text(separator=' ', strip=True)\n        if quoted_text:\n            quoted['quoted_text'] = quoted_text\n        if quoted:\n            result.update(quoted)\n\n    return result",
  "selector": "article[data-testid='tweet']",
  "selector_description": "the tweet containers, containing user info content and metrics"
}