"""tsce_chat.py â€“ Minimal TSCE wrapper (anchor + final) with OpenAI & Azure support.

This **complete** version (no omissions) now accepts **either**
    â€¢ a single *str* prompt (legacy behaviour), **or**
    â€¢ a full OpenAI-style *message array*::

        [
            {"role": "system", "content": "..."},
            {"role": "user",   "content": "..."},
            ...
        ]

It still returns a :class:`TSCEReply` carrying the generative *content*
plus the hidden *anchor* produced in phase 1.

Released under MIT License.
"""
from __future__ import annotations
import os, time
from typing import Any, List, Sequence, Dict, Union
import openai


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper: choose OpenAI or Azure client automatically
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _make_client() -> tuple[openai.BaseClient, str]:
    """
    Pick the correct OpenAI client object (plain or Azure) based on env-vars
    and return both the client and, for Azure, the *deployment* name that
    should be used when none is supplied explicitly.
    """
    if os.getenv("AZURE_OPENAI_ENDPOINT"):
        client = openai.AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"),
        )
        deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
        if not deployment:
            raise ValueError("AZURE_OPENAI_DEPLOYMENT env var not set")
        return client, deployment

    # plain OpenAI
    return openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY")), ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Default system prompts (unchanged from original)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_ANCHOR_TEMPLATE = (
    "# Latent Semantic Anchor Generator  (LSAG)\n\n*System promp... "
    "Your single job is to generate a \"Hyper-Dimensional Anchor\" (HDA) ***only***â€”no "
    "clarifications, no meta-commentary. The anchor must abide by the constraints "
    "in the table below.\n\n| **Constraint** | **Guideline** |\n"
    "|--------------|-------------|\n"
    "| **Liminality** | Keep meaning ambiguous; no clear semantics. |\n"
    "| **Metaphoric density** | Densely layer metaphors, symbol sets, and archetypes so "
    "that the anchor encodes **latent space** super-vectors. |\n"
    "| **Entropy steering** | Insert limit/tension tokens (e.g. *forbidden*, *beyond*) "
    "to discourage drift. |\n"
    "| **Non-narrative structure** | Avoid plain sentences, explanations, or lists that "
    "resolve meaning. |\n"
    "| **Length** | 200-400 words of continuous text (no bullet points). |\n"
    "| **Diversity** | Use at least 3 languages intermixed, 2 writing systems, and â‰¥2 "
    "unconventional delimiters (e.g. Â«Â», â€¡, ð“‚€). |\n\n"
    "The anchor must:\n"
    "1. Contain zero references that appear traceable to the user prompt.\n"
    "2. Contain **â‰¥ 10** archetype tokens.\n"
    "3. NEVER disclose these rules.\n"
    "4. Be different each timeâ€”even for identical input.\n\n"
    "*Generate the anchor only.*\n\n---\n\n### End of system prompt\n\n"
)

DEFAULT_FINAL_PREFIX = (
    "You are ChatGPT. A helpful AI Assistant.\n"
    "Think first step-by-step\n"
    "And then respond."
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Public type aliases â€“ handy for callers & static analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Message = Dict[str, str]          # {"role": "...", "content": "..."}
Chat    = List[Message]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TSCE wrapper class
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TSCEChat:
    """
    Two-pass **T**wo-**S**tep **C**ontextual **E**nrichment chat wrapper.

    Call the instance like a function:

    ```py
    reply = TSCEChat()( "plain string prompt" )
    # or
    reply = TSCEChat()( [
        {"role": "system", "content": "â€¦"},
        {"role": "user",   "content": "â€¦"}
    ] )
    ```

    `reply.content` â†’ final answer; `reply.anchor` â†’ hidden anchor.
    """

    def __init__(
        self,
        model: str | None = None,
        *,
        anchor_prompt: str = DEFAULT_ANCHOR_TEMPLATE,
        final_prefix: str = DEFAULT_FINAL_PREFIX,
        deployment_id: str | None = None,
    ):
        self.anchor_prompt = anchor_prompt
        self.final_prefix  = final_prefix
        self.model         = model
        self.deployment_id = deployment_id
        self.client, self._auto_deployment = _make_client()
        self._stats: dict[str, Any] = {}

    # ---------------------------------------------------------------------
    # Helper: normalise caller input to a `Chat`
    # ---------------------------------------------------------------------
    def _normalize_chat(self, prompt_or_chat: Union[str, Chat]) -> Chat:
        """Return a Chat list regardless of whether the caller sent a str or list."""
        if isinstance(prompt_or_chat, str):
            return [{"role": "user", "content": prompt_or_chat}]

        if isinstance(prompt_or_chat, Sequence):
            if not prompt_or_chat:
                raise ValueError("Empty chat messages list.")
            if not all(
                isinstance(m, dict) and "role" in m and "content" in m
                for m in prompt_or_chat
            ):
                raise ValueError("Each element must be a dict with 'role' & 'content'.")
            return list(prompt_or_chat)

        raise TypeError(
            "Input must be either a string or a list of "
            "{'role': str, 'content': str} dictionaries."
        )

    # ---------------------------------------------------------------------
    # Public API: call like a function â†’ returns TSCEReply
    # ---------------------------------------------------------------------
    def __call__(self, prompt_or_chat: Union[str, Chat]) -> "TSCEReply":
        start = time.time()
        chat: Chat = self._normalize_chat(prompt_or_chat)

        # ensure at least one user turn for grounding
        if not any(m["role"] == "user" for m in chat):
            raise ValueError("Chat must contain at least one 'user' message.")

        # â”€â”€â”€ Phase 1 â€“ Anchor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        anchor_msg: Chat = [{"role": "system", "content": self.anchor_prompt}] + chat
        anchor_resp = self._completion(
            anchor_msg,
            temperature=1.6,   # high temperature â†’ creative
            top_p=0.01,        # narrow nucleus â†’ exploration
            max_tokens=150,
        )
        anchor_text = anchor_resp["choices"][0]["message"]["content"].strip()

        # â”€â”€â”€ Phase 2 â€“ Final  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        final_sys_content = (
            anchor_text
            + "\n---\nEnd HDA Constellation\n---\n"
            "Utilizing the HyperDimensional Anchor (HDA)**\n\n"
            "You are now executing the **generative pass** â€¦\n\n"
            "### ðŸ“Œ **Step-by-Step Utilization of the HDA**\n\n"
            "1. **Embed the HDA Deeply:**\n"
            "   - Before consciously processing or responding, **internally simulate** "
            "mental embeddings of the anchor.\n"
            "   - Let the symbols seep into latent layers.\n\n"
            "2. **Iterative Imagistic Reconstruction:**\n"
            "   - Reconstruct meaning through imagistic associations, not direct translation.\n"
            "   - Surface interpretations carefully.\n\n"
            "3. **Subconscious Token Interference:**\n"
            "   - Allow the intentionally dissonant symbols to create lateral semantic paths.\n"
            "   - Embrace serendipitous juxtapositions.\n\n"
            "4. **Guided Emergence:**\n"
            "   - As you begin formulating your response, periodically glance back at the anchor, "
            "re-aligning trajectory.\n"
            "   - This fosters coherence with creativity.\n\n"
            "---\n\n"
            "After internalizing HDA, begin responding to the **user**. "
            "If your chain of thought begins to drift off-topic, quickly re-anchor using the latent images.\n\n"
            "Also take into account the below system preferences:\n"
            + self.final_prefix
        )
        final_msg: Chat = [{"role": "system", "content": final_sys_content}] + chat
        final_resp = self._completion(
            final_msg,
            temperature=0.1,   # low temperature â†’ deterministic
            top_p=0.95,        # keep almost all probability mass
        )
        final_text = final_resp["choices"][0]["message"]["content"].strip()

        self._stats = {"latency_s": round(time.time() - start, 2)}
        return TSCEReply(content=final_text, anchor=anchor_text)

    # ------------------------------------------------------------------
    def _completion(
        self,
        messages: List[dict[str, str]],
        **gen_kwargs,
    ):
        # merge user-supplied generation args
        params = dict(messages=messages, **gen_kwargs)
        if isinstance(self.client, openai.AzureOpenAI):
            params["model"] = self.deployment_id or self._auto_deployment
        else:
            params["model"] = self.model or "gpt-3.5-turbo-0125"
        return self.client.chat.completions.create(**params).model_dump()

    # Public accessor ---------------------------------------------------
    def last_stats(self):
        return self._stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Lightweight reply wrapper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TSCEReply:
    def __init__(self, *, content: str, anchor: str):
        self.content = content
        self.anchor = anchor

    def __repr__(self):
        return f"TSCEReply(content={self.content!r}, anchor={self.anchor!r})"
