{
  "python": "from bs4 import BeautifulSoup\nimport re\n\ndef parse_element(html: str) -> dict:\n    result = {}\n    try:\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # ID\n        for attr in ('data-id', 'id'):\n            id_val = soup.attrs.get(attr)\n            if id_val:\n                result['id'] = id_val\n                break\n\n        # Title - Only add if non-empty and distinct from handle\n        title_tag = None\n        for name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n            title_tag = soup.find(name)\n            if title_tag:\n                break\n        if not title_tag:\n            title_tag = soup.find(attrs={'aria-label': True})\n        if title_tag and title_tag.get_text(strip=True):\n            result['title'] = title_tag.get_text(strip=True)\n\n        # Author info: try profile (display) name and handle (URL)\n        author = None\n        author_url = None\n        # Try to find canonical author name from visible content\n        display_candidates = []\n        for span in soup.find_all('span'):\n            text = span.get_text(strip=True)\n            if text and not re.search(r'^(@|Follows you|Reply|Retweet|Like|View|Follow|Verified|\\.{2,})$', text, re.I):\n                display_candidates.append(text)\n        if display_candidates:\n            # Heuristic: Twitter's display name is usually among first two\n            author = display_candidates[0]\n        # Author URL: the first /user handle href anchor (role=link or aria-label)\n        handle_a = None\n        for a in soup.find_all('a', href=True):\n            if re.match(r'^/[^/]+$', a['href']):\n                handle_a = a\n                break\n        if handle_a:\n            author_url = handle_a['href']\n        # Clean up duplication\n        if author and author_url and author == author_url:\n            author = None\n        if author:\n            result['author'] = author\n        if author_url:\n            result['author_url'] = author_url\n\n        # Primary URL: choose canonical URL to user or content (skip if same as author_url)\n        primary_url = None\n        time_tag = soup.find('time', attrs={'datetime': True})\n        if time_tag and time_tag.parent.name == 'a':\n            primary_url = time_tag.parent.get('href')\n        if not primary_url:\n            main_links = [a['href'] for a in soup.find_all('a', href=True)]\n            if main_links:\n                for lnk in main_links:\n                    if not (author_url and lnk == author_url):\n                        primary_url = lnk\n                        break\n                if not primary_url:\n                    primary_url = main_links[0]\n        # Remove duplication between author_url/primary_url if targets are same\n        if primary_url and author_url and primary_url == author_url:\n            primary_url = None\n        if primary_url:\n            result['primary_url'] = primary_url\n\n        # All other URLs (exclude author_url, primary_url)\n        all_urls = set(a['href'] for a in soup.find_all('a', href=True))\n        for key in ('primary_url', 'author_url'):\n            if key in result and result[key] in all_urls:\n                all_urls.remove(result[key])\n        if all_urls:\n            result['urls'] = list(all_urls)\n\n        # Description: extract distinct main text blocks (skip full get_text(); filter metadata/action/handles)\n        # Find longest <div> or <span> or <p> with normal color (not gray), exclude spans tied to actions\n        desc_cands = []\n        for tag in soup.find_all(['div', 'p', 'span']):\n            txt = tag.get_text(' ', strip=True)\n            if (\n                txt and len(txt) > 25\n                and not re.search(r'^(Reply|Like|Retweet|Share|View|Bookmark|Follow|Verified|Follows you|@[\\w_-]+|Click to\\\\s+Follow|Click to\\\\s+Unfollow|Following|Follow back|All views mine\\\\.?|\\\\.\\\\.\\\\.)', txt, re.I)\n                and not (author and txt == author)\n                and not (author_url and txt == author_url)\n            ):\n                desc_cands.append(txt)\n        # Deduplicate/sort\n        desc_cands = sorted(set(desc_cands), key=len, reverse=True)\n        # Remove superset-of-full-element cases that match soup.get_text()\n        text_whole = soup.get_text(' ', strip=True)\n        if desc_cands and len(desc_cands) > 1 and desc_cands[0] == text_whole:\n            desc_cands = desc_cands[1:]\n        # Additional filter: must contain more than just URLs/handles/keywords\n        if desc_cands:\n            if not re.fullmatch(r'(?:@[\\w_-]+|https?://[^ ]+|,|\\\\.|\\\\s)+', desc_cands[0]):\n                result['description'] = desc_cands[0]\n\n        # Images - Filter profile avatars and emojis\n        images = []\n        for img in soup.find_all('img', src=True):\n            src = img['src']\n            if src.startswith('data:'):\n                continue\n            # Heuristic: Filter obvious emoji SVG (twemoji)\n            if 'emoji' in src and 'svg' in src:\n                pass # let it through; removes further heuristics in real app\n            img_info = {'src': src}\n            for attr in ['alt', 'title', 'width', 'height']:\n                val = img.get(attr)\n                if val:\n                    img_info[attr] = val\n            for k, v in img.attrs.items():\n                if k.startswith('data-'):\n                    img_info[k] = v\n            images.append(img_info)\n        if images:\n            result['images'] = images\n\n        # Timestamps\n        time_tag = soup.find('time')\n        if time_tag:\n            timestamp = time_tag.text.strip()\n            if timestamp:\n                result['timestamp'] = timestamp\n            dt = time_tag.get('datetime')\n            if dt:\n                result['datetime'] = dt\n\n        # Metrics: as previously\n        metric_map = {\n            'reply_count': ['reply', 'comment'],\n            'repost_count': ['repost', 'retweet'],\n            'like_count': ['like', 'fave', 'favorite'],\n            'bookmark_count': ['bookmark'],\n            'view_count': ['view'],\n        }\n        for k, keywords in metric_map.items():\n            val = None\n            for kw in keywords:\n                tag = soup.find(attrs={'aria-label': re.compile(kw, re.I)})\n                if tag:\n                    text = tag.get_text(strip=True)\n                    m = re.search(r'([\\d,.]+[KM]?)', text)\n                    if not m:\n                        next_text = ''\n                        if tag.next_sibling:\n                            next_text = getattr(tag.next_sibling, 'get_text', lambda **a: str(tag.next_sibling))().strip()\n                        m = re.search(r'([\\d,.]+[KM]?)', next_text)\n                    if m:\n                        val = m.group(1)\n                        break\n            if not val:\n                tag = soup.find(attrs={'data-testid': re.compile('|'.join(keywords), re.I)})\n                if tag:\n                    number = re.search(r'([\\d,.]+[KM]?)', tag.get_text(strip=True))\n                    if number:\n                        val = number.group(1)\n            if val:\n                num = val.replace(',', '')\n                if num.endswith('K'):\n                    num = int(float(num[:-1])*1000)\n                elif num.endswith('M'):\n                    num = int(float(num[:-1])*1000000)\n                else:\n                    try:\n                        num = int(num)\n                    except Exception:\n                        continue\n                result[k] = num\n\n        # Rank\n        rank_val = None\n        if 'data-rank' in soup.attrs:\n            rank_val = soup.attrs['data-rank']\n        else:\n            tag = soup.find(attrs={'data-rank': True})\n            if tag:\n                rank_val = tag['data-rank']\n        if rank_val:\n            try:\n                result['rank'] = int(rank_val)\n            except Exception:\n                pass\n\n        # Quoted content\n        quote_tag = soup.find(attrs={'data-testid': re.compile('quote|quoted', re.I)})\n        if quote_tag:\n            quoted = {}\n            author_tag = quote_tag.find('a', href=True)\n            if author_tag:\n                quoted['quoted_author_name'] = author_tag.get_text(strip=True)\n                quoted['quoted_url'] = author_tag['href']\n            quoted_txt_tag = quote_tag.find('p')\n            if quoted_txt_tag:\n                quoted['quoted_text'] = quoted_txt_tag.get_text(strip=True)\n            if quoted:\n                result['quoted'] = quoted\n\n    except Exception:\n        pass\n    # Remove any empty, blank, or None values\n    clean_result = {k:v for k,v in result.items() if (v if not isinstance(v, str) else v.strip()) or (isinstance(v, int) and v == 0)}\n    return clean_result",
  "selector": "div[data-testid='cellInnerDiv'] button[data-testid='UserCell']",
  "selector_description": "All verified followers listed in the user's follower list with profile names, handles, bios, and follow status"
}