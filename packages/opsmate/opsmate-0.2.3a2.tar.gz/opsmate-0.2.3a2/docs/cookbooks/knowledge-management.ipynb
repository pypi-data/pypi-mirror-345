{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this cookbook, we will learn how to use Opsmate to manage knowledge.\n",
                "\n",
                "Notes the knowledge management feature is currently in the early stage of development, the features and the UX are subject to change.\n",
                "At the moment 2 type of data source can be ingested as knowledge:\n",
                "\n",
                "1. Any text based files from your local file system or network-attached storage.\n",
                "2. Any text based files from Github repositories.\n",
                "\n",
                "We use [lancedb](https://lancedb.github.io/lancedb/) as the underlying vector database to store the knowledge. We use lancedb mainly because of the serverless nature of the database where you can use the cloud storage as the backend, which reduces the cost of ownership.\n",
                "\n",
                "Knowledge retrieval is can be achieved via the `KnowledgeRetrival` tool - which is a built-in tool in Opsmate."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment variablebased configuration Options\n",
                "\n",
                "### FS_EMBEDDINGS_CONFIG\n",
                "\n",
                "This is a JSON key-value pair where the key is the path to the directory to be ingested and the value is the glob pattern to match the files to be ingested.\n",
                "\n",
                "Example usage:\n",
                "\n",
                "```\n",
                "FS_EMBEDDINGS_CONFIG='{\"./docs/cookbooks\": \"*.md\"}'\n",
                "```\n",
                "\n",
                "This will ingest all the markdown files in the `./docs/cookbooks` directory.\n",
                "\n",
                "### GITHUB_EMBEDDINGS_CONFIG\n",
                "\n",
                "This is a JSON key-value pair where the key is the `owner/repo:optional[branch]` and the value is the glob pattern to match the files to be ingested.\n",
                "\n",
                "Example usage:\n",
                "\n",
                "```\n",
                "GITHUB_EMBEDDINGS_CONFIG='{\"opsmate/opsmate\": \"*.md\", \"kubernetes/kubernetes:test-branch\": \"*.txt\"}'\n",
                "```\n",
                "\n",
                "In the example above, the first entry will ingest all the markdown files in the `opsmate/opsmate` repository. The second entry will ingest all the text files in the `kubernetes/kubernetes` repository on the `test-branch` branch.\n",
                "\n",
                "If the branch is not specified, it will default to `main`.\n",
                "\n",
                ":important: The Github token is required to be set in the environment variable `GITHUB_TOKEN`.\n",
                "\n",
                "\n",
                "### EMBEDDING_REGISTRY_NAME and EMBEDDING_MODEL_NAME\n",
                "\n",
                "`EMBEDDING_REGISTRY_NAME` is the name of the embedding registry to use. It is default to `openai`.\n",
                "\n",
                "`EMBEDDING_MODEL_NAME` is the name of the embedding model to use. It is default to `text-embedding-ada-002`.\n",
                "\n",
                "LanceDB supports wide range of embedding models, you can refer to the [lancedb embedding documentation](https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/#text-embedding-functions) for more details.\n",
                "\n",
                "### EMBEDDINGS_DB_PATH\n",
                "\n",
                "EMBEDDINGS_DB_PATH is the path to the lancedb database. It is default to `~/.data/opsmate-embeddings`.\n",
                "\n",
                "Right now it is defaulted to the local file system, but there are wide range of storage options supported by lancedb, you can refer to the [lancedb storage documentation](https://lancedb.github.io/lancedb/concepts/storage/) for more details. In the documentation it provides a very comprehensive diagram to show case the thought process that goes into choosing the right storage backend.\n",
                "\n",
                "**WARNING**: Currently the ingestion chunk size is set to 1000, with overlap set to 0, with recursive text splitter as the default chunking strategy. This is hardcoded right now through environment-variable based configuration, but we will support more flexible configuration in the future.\n",
                "\n",
                "### SPLITTER_CONFIG\n",
                "\n",
                "Currently there are 2 types of splitter:\n",
                "\n",
                "1. RecursiveTextSplitter\n",
                "2. MarkdownHeaderTextSplitter\n",
                "\n",
                "Here are the example configurations:\n",
                "\n",
                "```bash\n",
                "SPLITTER_CONFIG='{\"name\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 0}' # this is the default configuration\n",
                "\n",
                "# OR\n",
                "\n",
                "SPLITTER_CONFIG='{\"name\": \"markdown_header\", \"headers_to_split_on\": [[\"#\", \"h1\"], [\"##\", \"h2\"], [\"###\", \"h3\"]]}'\n",
                "```\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SDK-based data ingestion\n",
                "\n",
                "You can also choose to ingest the knowledge via the SDK which provides greater flexibility in terms of configuration.\n",
                "\n",
                "In the example below, we ingest all the markdown files in the `docs/book/src` directory of the `kubernetes-sigs/kubebuilder` repository to learn about the kubebuilder.\n",
                "\n",
                "Note this is going to take a while to complete and emit a lot of logs so we are not going to run it here.\n",
                "\n",
                "```python\n",
                "from opsmate.config import config\n",
                "import asyncio\n",
                "from sqlmodel import create_engine, text\n",
                "import structlog\n",
                "from opsmate.app.base import on_startup as base_app_on_startup\n",
                "from opsmate.ingestions import ingest_from_config\n",
                "from opsmate.config import Config\n",
                "\n",
                "logger = structlog.get_logger()\n",
                "\n",
                "\n",
                "async def main():\n",
                "    engine = create_engine(\n",
                "        config.db_url,\n",
                "        connect_args={\"check_same_thread\": False},\n",
                "        # echo=True,\n",
                "    )\n",
                "    with engine.connect() as conn:\n",
                "        conn.execute(text(\"PRAGMA journal_mode=WAL\"))\n",
                "        conn.close()\n",
                "\n",
                "    await base_app_on_startup(engine)\n",
                "\n",
                "    await ingest_from_config(\n",
                "        Config(\n",
                "            github_embeddings_config={\n",
                "                \"kubernetes-sigs/kubebuilder:master\": \"./docs/book/src/**/*.md\"\n",
                "            },\n",
                "            categorise=False,  # By default we categorise the knowledge into categories for better segmentation, but we disable it here for the sake of speed.\n",
                "        ),\n",
                "        engine=engine,\n",
                "    )\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    asyncio.run(main())\n",
                "```\n",
                "\n",
                "You can initiate the ingestion via `OPSMATE_DB_URL=sqlite:////tmp/sqlite.db python main.py`\n",
                "\n",
                "For the actual ingestion, start the background worker via `OPSMATE_DB_URL=sqlite:////tmp/sqlite.db python -m opsmate.dbqapp.app`\n",
                "\n",
                "Once the knowledge of the kubebuilder is ingested, we can use the `KnowledgeRetrieval` tool to provide retrieval augmented generation (RAG):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2m2025-02-21 17:02:04\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrunning knowledge retrieval tool\u001b[0m \u001b[36mquery\u001b[0m=\u001b[35mhow to do env test against a real cluster in kubebuilder using environment variables?\u001b[0m\n",
                        "To run envtest against a real cluster using Kubebuilder, you need to set specific environment variables to point to the existing cluster's control plane and binaries. Here are the key environment variables to use:\n",
                        "\n",
                        "1. **`USE_EXISTING_CLUSTER`**: Set this to `true` to connect to an existing cluster instead of creating a local control plane.\n",
                        "\n",
                        "2. **`KUBEBUILDER_ASSETS`**: This should point to the directory containing the binaries needed for your tests (like `kubectl`, `etcd`, and `kube-apiserver`).\n",
                        "\n",
                        "3. **`TEST_ASSET_KUBE_APISERVER`, `TEST_ASSET_ETCD`, `TEST_ASSET_KUBECTL`**: These variables can be set to the specific paths of the `kube-apiserver`, `etcd`, and `kubectl` binaries, respectively. They provide a more granular way to specify which binaries to use if they differ from the default ones.\n",
                        "\n",
                        "### Example of Setting Variables\n",
                        "You can export the necessary variables in your terminal session before running your tests:\n",
                        "```bash\n",
                        "export USE_EXISTING_CLUSTER=true\n",
                        "export KUBEBUILDER_ASSETS=\"/path/to/binaries/\"\n",
                        "export TEST_ASSET_KUBE_APISERVER=\"/path/to/kube-apiserver\"\n",
                        "export TEST_ASSET_ETCD=\"/path/to/etcd\"\n",
                        "export TEST_ASSET_KUBECTL=\"/path/to/kubectl\"\n",
                        "```\n",
                        "After setting these environment variables, you can run your tests, and they will utilize the existing cluster rather than initializing a new one.\n"
                    ]
                }
            ],
            "source": [
                "from opsmate.tools import KnowledgeRetrieval\n",
                "\n",
                "result = await KnowledgeRetrieval(query=\"how to do env test against a real cluster in kubebuilder using environment variables?\").run()\n",
                "\n",
                "print(result.summary)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The question is fairly obscure, which in the past took me several hours to figure out with the help from mixture of google search and reading the kubebuilder documentation.\n",
                "\n",
                "With semantic search, the answer is returned in seconds.\n",
                "\n",
                "## Future capabilities\n",
                "\n",
                "* Right now the async based knowledge ingestion is fairly naive and is not designed to be run in a distributed and fault-tolerant manner. We need to design a more robust system to support this - Potentially brining in the big gun such as [Celery](https://docs.celeryq.dev/en/stable/) but ideally anything easy to maintain and scale.\n",
                "* We need to support more data source types, such as databases or other API-based data sources.\n",
                "* Currently only text-based files are supported, we need to support more file types, such as images, videos, and other binary data.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
