{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "\n",
    "import pose_pipeline\n",
    "pose_pipeline.set_environmental_variables()  # allows running the pose pipeline\n",
    "from pose_pipeline.utils.jupyter import play\n",
    "from pose_pipeline.pipeline import *\n",
    "\n",
    "# load this specific schema\n",
    "from pose_pipeline.demo.h36_dj import *\n",
    "\n",
    "# NOTE: adjust to your installation\n",
    "os.environ['CDF_LIB'] = '/home/jcotton/CDF/cdf38_0-dist/src/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this script https://github.com/anibali/h36m-fetch to download human3.6 and unpack it. \n",
    "# Please follow the Human 3.6 license.\n",
    "\n",
    "video_project = 'h36m'\n",
    "human36_extracted_directory = '/mnt/data0/datasets/human3.6/h36m-fetch/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a39094-4fbe-43f5-9100-1c9dbb3b6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the data into PosePipe\n",
    "import_data(human36_extracted_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cf5b9-91a3-47c7-a1ba-b14071615efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show frame from random video\n",
    "key = (Video & 'video_project=\"h36m\"').fetch('KEY')[0]\n",
    "video = (Video & key).fetch1('video')\n",
    "cap = cv2.VideoCapture(video)\n",
    "ret, frame = cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "cap.release()\n",
    "os.delete(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fbb76-d55f-4898-8614-536ea76dadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(frame)\n",
    "plt.plot(poses2d[0, :, 0], poses2d[0, :, 1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49082066-10a1-48ce-ac1f-2527b60bf71e",
   "metadata": {},
   "source": [
    "# Now process them with pipeline code\n",
    "\n",
    "Alternative can use 'scripts/process_h36m.py' to run from CLI and easily run multiple scripts on different GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c93434-f3d8-4998-abbb-ba69ad353ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_pipeline.utils.standard_pipelines import top_down_pipeline\n",
    "#james_walking_sync_20220830_173523 james_walking_sync_20220830_173351\n",
    "#homeleg_stairs_20220901_120001\n",
    "\n",
    "VIDEO_PROJECT = \"h36m\"\n",
    "keys = (Video & f'video_project=\"{VIDEO_PROJECT}\"').fetch('KEY')\n",
    "\n",
    "for k in keys:\n",
    "    #top_down_pipeline(k, top_down_method_name=\"OpenPose\", tracking_method_name='DeepSortYOLOv4')\n",
    "    top_down_pipeline(k, top_down_method_name=\"MMPoseHalpe\", tracking_method_name='DeepSortYOLOv4')\n",
    "    #top_down_pipeline(k, top_down_method_name=\"MMPoseHalpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534631c-7699-4d37-be52-ac5bfa41f34d",
   "metadata": {},
   "source": [
    "# Annotation (when multiple people detected)\n",
    "\n",
    "Check for unannotated videos and run annotation gui. Then rerun the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a5f58-c17b-4ed2-89d7-2295630b3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to check assignment for ones not automatically done\n",
    "\n",
    "filt = 'video_project=\"h36m\"'\n",
    "\n",
    "BlurredVideo.populate((TrackingBbox - PersonBbox) & filt, suppress_errors=False)\n",
    "TrackingBboxVideo.populate(BlurredVideo - PersonBbox & filt, suppress_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b13bb9-7573-4c50-9c1c-1ec019e613af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run GUI\n",
    "\n",
    "from pose_pipeline.utils.annotation import annotate\n",
    "\n",
    "to_label = ((TrackingBboxVideo & filt) - PersonBboxValid).fetch('KEY')\n",
    "print(f'{len(to_label)} found to annotate')\n",
    "\n",
    "key = to_label[0]\n",
    "\n",
    "annotate(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd879964-632f-4a81-af1c-1eef897b0c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
