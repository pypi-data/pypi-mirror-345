#@meta {desc: "HuggingFace trainer config", date: "2025-02-11"}


## Resource
#
lmtask_trainer_hf_bnb_config:
  class_name: transformers.BitsAndBytesConfig
  load_in_4bit: true
  # quantize to 4-bit normalized float
  bnb_4bit_quant_type: 'nf4'
  bnb_4bit_compute_dtype: 'float16'
  # apply double quantization for constants
  bnb_4bit_use_double_quant: true

lmtask_trainer_hf_source_model_args:
  quantization_config: 'instance: lmtask_trainer_hf_bnb_config'
  device_map: auto
  # do not reuse the query,key,value computations from previous tokens; if set
  # to true, it would speed up the decoding process
  use_cache: false
  # setting pretraining_tp (temperature) to a value different than 1 will
  # activate the more accurate but slower computation of the linear layers,
  # which should better match the original logits
  pretraining_tp: 1

lmtask_trainer_hf_source_resource:
  model_id: ${lmtask_trainer_default:source_model}
  model_args: 'asdict: lmtask_trainer_hf_source_model_args'

lmtask_trainer_hf_peft:
  class_name: peft.LoraConfig
  # choose any number > 0; suggested 8, 16, 32, 64, 128
  r: 64
  # add for continual pretraining
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens
    - lm_head
  lora_alpha: 32
  # supports any, but 0 is optimized
  lora_dropout: 0.1
  # supports any, but 'none' is optimized
  bias: 'none'
  # Overview of the supported task types:
  #   - SEQ_CLS: Text classification.
  #   - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.
  #   - Causal LM: Causal language modeling.
  #   - TOKEN_CLS: Token classification.
  #   - QUESTION_ANS: Question answering.
  #   - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features
  #     for downstream tasks.
  task_type: 'CAUSAL_LM'

lmtask_trainer_hf_train_resource:
  class_name: zensols.lmtask.hf.HFTrainerResource
  generator_resource: 'instance: lmtask_trainer_hf_source_resource'
  peft_config: 'instance: lmtask_trainer_hf_peft'


## HuggingFace trainer
#
lmtask_trainer_hf_training_arguments:
  class_name: transformers.TrainingArguments
  output_dir: 'path: ${lmtask_trainer_default:checkpoint_dir}'
  # load 2 batches into gpu ram at each step
  per_device_train_batch_size: 2
  # after 2 steps (since 2 batches per step), we will reach 4 batches
  # accumulated so now update weights for next gradient decision.
  gradient_accumulation_steps: 8
  # ratio of total training steps used for a linear warmup from 0 to learning_rate
  warmup_ratio: 0.1
  optim: 'paged_adamw_32bit'
  # learning rate
  learning_rate: 5e-5
  # learning rate will decrease over time to make convergence faster
  lr_scheduler_type: 'cosine'
  # total number of training epochs to perform (if not an integer, will perform
  # the decimal part percents of the last epoch before stopping training)
  num_train_epochs: 1
  # If set to a positive number, the total number of training steps to
  # perform. Overrides num_train_epochs. For a finite dataset, training is
  # reiterated through the dataset (if all data is exhausted) until max_steps
  # is reached
  #max_steps: 1
  # Logging is done every logging_steps
  logging_steps: 10
  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit
  # training.
  fp16: true
  # save model weights every time weights are updated (which is after 2 steps
  # after 4 batches are accumulated)
  gradient_checkpointing: true
  seed: 0
  # used for WandB
  report_to: 'none'

lmtask_trainer_hf_trainer_arguments:
  max_seq_length: ${lmtask_trainer_default:max_seq_length}
  args: 'instance: lmtask_trainer_hf_training_arguments'
  dataset_num_proc: 8
  peft_config: 'instance: lmtask_trainer_hf_peft'

lmtask_trainer_hf:
  class_name: zensols.lmtask.hf.HuggingFaceTrainer
  resource: 'instance: lmtask_trainer_hf_train_resource'
  trainer_params: 'asdict: lmtask_trainer_hf_trainer_arguments'
