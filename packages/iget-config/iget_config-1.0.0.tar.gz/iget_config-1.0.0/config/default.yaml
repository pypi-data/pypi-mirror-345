train:
  batch_size: 256
  num_epochs: 50
  num_workers: 0
  shuffle: true
  log_mode: true
  graph_rate: 10

data:
  window_size: 6
  label_size: 1
  label_exclude_days: 1
  label_mode: 2
  train_days: 1000
  validate_days: 240
  test_days: 30
  feature_size: 12
  feature_mode: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  label_bins: [-0.08, -0.05, -0.02, -0.01, 0.01, 0.02, 0.05, 0.10]
  feature_bins: [
            [-0.08, -0.05, -0.02, -0.01, 0.01, 0.02, 0.05, 0.10],
            [0.00, 0.01, 0.01, 0.02, 0.04],
            [0.00, 0.01, 0.01, 0.02, 0.03],
            [-0.10, -0.05, -0.02, -0.01, 0.01, 0.02, 0.06, 0.10],
            [0.00, 0.01, 0.02, 0.03, 0.06, 0.15],
            [-0.57, -0.44, -0.28, -0.13, 0.05, 0.35, 1.08, 2.55],
            [-0.04, -0.02, -0.01, -0.01, 0.01, 0.01, 0.02, 0.04],
            [4.04, 14.97, 40.05, 102.29, 271.57, 1258.67],
            [1.50, 2.50],
            [0.00, 0.21, 0.47, 0.75, 1.00],
            [0.00, 0.08, 0.25, 0.45, 0.67, 0.87, 1.00],
            [0.03, 0.12, 0.26, 0.42, 0.60, 0.78, 0.92, 1.00],
            [0.06, 0.13, 0.25, 0.38, 0.53, 0.71, 0.86, 0.95],
            [0.00, 0.14, 0.30, 0.50, 0.82, 1.00],
            [0.00, 0.06, 0.14, 0.24, 0.37, 0.57, 0.82, 1.00],
            [0.02, 0.06, 0.11, 0.18, 0.27, 0.41, 0.60, 0.79],
            [0.02, 0.04, 0.08, 0.12, 0.18, 0.29, 0.43, 0.58],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-10.00, -4.00, -1.00, -0.50, -0.10, 0.10, 0.50, 1.00, 4.00, 10.00],
            [-0.12, -0.07, -0.03, -0.01, 0.01, 0.03, 0.07, 0.16],
            [-0.19, -0.12, -0.06, -0.02, 0.01, 0.05, 0.13, 0.28],
            [-0.28, -0.18, -0.10, -0.04, 0.02, 0.09, 0.24, 0.48],
            [-0.37, -0.27, -0.15, -0.07, 0.02, 0.14, 0.40, 0.80],
            [-0.57, -0.45, -0.29, -0.15, 0.01, 0.24, 0.76, 1.49],
            [-0.69, -0.57, -0.39, -0.22, -0.02, 0.31, 1.12, 2.48],
            [-0.77, -0.67, -0.50, -0.30, -0.06, 0.36, 1.45, 3.35],
            [-0.84, -0.74, -0.58, -0.38, -0.13, 0.35, 1.63, 3.92],
            [-0.06, -0.03, -0.01, -0.01, 0.01, 0.01, 0.03, 0.05],
            [-0.02, -0.01, -0.01, -0.01, 0.01, 0.01, 0.02, 0.03],
            [-0.28, -0.19, -0.09, -0.03, 0.03, 0.10, 0.24, 0.46],
        ]
  loss_weight: [0.2357, 0.1966, 0.025, 0.0284, 0.0266, 0.1852, 0.3026]
  max_stop_rate: 0.3

Adam:
  lr: 0.001
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01
  amsgrad: false

OneCycleLR:
  max_lr: 0.003
  # total_steps: null # 期望外部计算
  # epochs: 50         # 从 OneCycleLR 节移除
  # steps_per_epoch: null
  pct_start: 0.3
  anneal_strategy: 'cos'
  div_factor: 25.0
  final_div_factor: 10000.0
  three_phase: false
  last_epoch: -1
  verbose: false
