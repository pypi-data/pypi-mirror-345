# Web Crawler MCP

[![English](https://img.shields.io/badge/lang-en-blue.svg)](../README.md) [![‰∏≠Êñá](https://img.shields.io/badge/lang-zh-blue.svg)](README.zh.md) [![‡§π‡§ø‡§Ç‡§¶‡•Ä](https://img.shields.io/badge/lang-hi-blue.svg)](README.hi.md) [![Espa√±ol](https://img.shields.io/badge/lang-es-blue.svg)](README.es.md) [![Fran√ßais](https://img.shields.io/badge/lang-fr-blue.svg)](README.fr.md) [![ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://img.shields.io/badge/lang-ar-blue.svg)](README.ar.md) [![‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](https://img.shields.io/badge/lang-bn-blue.svg)](README.bn.md) [![–†—É—Å—Å–∫–∏–π](https://img.shields.io/badge/lang-ru-blue.svg)](README.ru.md) [![Portugu√™s](https://img.shields.io/badge/lang-pt-blue.svg)](README.pt.md) [![Bahasa Indonesia](https://img.shields.io/badge/lang-id-blue.svg)](README.id.md)

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)

Un puissant outil de crawling web qui s'int√®gre avec des assistants IA via le MCP (Machine Conversation Protocol). Ce projet vous permet de crawler des sites web et de sauvegarder leur contenu [...]

## üìã Fonctionnalit√©s

- Crawling de sites web avec profondeur configurable
- Support pour liens internes et externes
- G√©n√©ration de fichiers Markdown structur√©s
- Int√©gration native avec les assistants IA via MCP
- Statistiques d√©taill√©es des r√©sultats de crawl
- Gestion des erreurs et des pages non trouv√©es

## üöÄ Installation

### Pr√©requis

- Python 3.9 ou sup√©rieur

### √âtapes d'installation

1. Cloner ce d√©p√¥t:

```bash
git clone laurentvv/crawl4ai-mcp
cd crawl4ai-mcp
```

2. Cr√©er et activer un environnement virtuel:

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/MacOS
python -m venv .venv
source .venv/bin/activate
```

3. Installer les d√©pendances requises:

```bash
pip install -r requirements.txt
```

## üîß Configuration

### Configuration MCP pour les Assistants IA

Pour utiliser ce crawler avec des assistants IA comme VScode Cline, configurez votre fichier `cline_mcp_settings.json`:

```json
{
  "mcpServers": {
    "crawl": {
      "command": "PATH\\TO\\YOUR\\ENVIRONMENT\\.venv\\Scripts\\python.exe",
      "args": [
        "PATH\\TO\\YOUR\\PROJECT\\crawl_mcp.py"
      ],
      "disabled": false,
      "autoApprove": [],
      "timeout": 600
    }
  }
}
```

Remplacez `PATH\\TO\\YOUR\\ENVIRONMENT` et `PATH\\TO\\YOUR\\PROJECT` par les chemins appropri√©s sur votre syst√®me.

#### Exemple concret (Windows)

```json
{
  "mcpServers": {
    "crawl": {
      "command": "C:\\Python\\crawl4ai-mcp\\.venv\\Scripts\\python.exe",
      "args": [
        "D:\\Python\\crawl4ai-mcp\\crawl_mcp.py"
      ],
      "disabled": false,
      "autoApprove": [],
      "timeout": 600
    }
  }
}
```

## üñ•Ô∏è Utilisation

### Utilisation avec un Assistant IA (via MCP)

Une fois configur√© dans votre assistant IA, vous pouvez utiliser le crawler en demandant √† l'assistant d'effectuer un crawl en utilisant la syntaxe suivante:

```
Pouvez-vous crawler le site web https://example.com avec une profondeur de 2?
```

L'assistant utilisera le protocole MCP pour ex√©cuter l'outil de crawling avec les param√®tres sp√©cifi√©s.

### Exemples d'utilisation avec Claude

Voici des exemples de demandes que vous pouvez faire √† Claude apr√®s avoir configur√© l'outil MCP:

- **Crawl simple**: "Pouvez-vous crawler le site example.com et me donner un r√©sum√©?"
- **Crawl avec options**: "Pouvez-vous crawler https://example.com avec une profondeur de 3 et inclure les liens externes?"
- **Crawl avec sortie personnalis√©e**: "Pouvez-vous crawler le blog example.com et sauvegarder les r√©sultats dans un fichier nomm√© 'blog_analysis.md'?"

## üìÅ Structure des r√©sultats

Les r√©sultats du crawl sont sauvegard√©s dans le dossier `crawl_results` √† la racine du projet. Chaque fichier de r√©sultat est au format Markdown avec la structure suivante:

```markdown
# https://example.com/page

## M√©tadonn√©es
- Profondeur: 1
- Horodatage: 2023-07-01T12:34:56

## Contenu
Contenu extrait de la page...

---
```

## üõ†Ô∏è Param√®tres disponibles

L'outil de crawl accepte les param√®tres suivants:

| Param√®tre | Type | Description | Valeur par d√©faut |
|-----------|------|-------------|---------------|
| url | cha√Æne | URL √† crawler (requis) | - |
| max_depth | entier | Profondeur maximale de crawling | 2 |
| include_external | bool√©en | Inclure les liens externes | false |
| verbose | bool√©en | Activer la sortie d√©taill√©e | true |
| output_file | cha√Æne | Chemin du fichier de sortie | g√©n√©r√© automatiquement |

## üìä Format des r√©sultats

L'outil renvoie un r√©sum√© avec:
- URL crawl√©e
- Chemin vers le fichier g√©n√©r√©
- Dur√©e du crawl
- Statistiques sur les pages trait√©es (r√©ussies, √©chou√©es, non trouv√©es, acc√®s interdit)

Les r√©sultats sont sauvegard√©s dans le r√©pertoire `crawl_results` de votre projet.

## ü§ù Contribution

Les contributions sont les bienvenues! N'h√©sitez pas √† ouvrir une issue ou √† soumettre une pull request.

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier LICENSE pour plus de d√©tails.