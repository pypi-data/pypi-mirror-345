"""
    Sifflet Backend API

    Requirements: <br>    - [Create your access token through the UI](https://docs.siffletdata.com/docs/generate-an-api-token#create-an-api-token) <br>    - Get your tenant name: if you access to Sifflet with `https://abcdef.siffletdata.com`, then your tenant would be `abcdef`  # noqa: E501

    The version of the OpenAPI document: 1.0.0
    Generated by: https://openapi-generator.tech
"""

import re  # noqa: F401
import sys  # noqa: F401

from sifflet_sdk.client.exceptions import ApiAttributeError
from sifflet_sdk.client.model_utils import (  # noqa: F401
    ApiTypeError,
    ModelComposed,
    ModelNormal,
    ModelSimple,
    OpenApiModel,
    cached_property,
    change_keys_js_to_python,
    convert_js_args_to_python_args,
    date,
    datetime,
    file_type,
    none_type,
    validate_get_composed_info,
)


def lazy_import():
    from sifflet_sdk.client.model.git_connection import GitConnection
    from sifflet_sdk.client.model.public_airflow_parameters_dto import (
        PublicAirflowParametersDto,
    )
    from sifflet_sdk.client.model.public_athena_parameters_dto import (
        PublicAthenaParametersDto,
    )
    from sifflet_sdk.client.model.public_big_query_parameters_dto import (
        PublicBigQueryParametersDto,
    )
    from sifflet_sdk.client.model.public_databricks_parameters_dto import (
        PublicDatabricksParametersDto,
    )
    from sifflet_sdk.client.model.public_dbt_cloud_parameters_dto import (
        PublicDbtCloudParametersDto,
    )
    from sifflet_sdk.client.model.public_dbt_parameters_dto import (
        PublicDbtParametersDto,
    )
    from sifflet_sdk.client.model.public_declarative_parameters_dto import (
        PublicDeclarativeParametersDto,
    )
    from sifflet_sdk.client.model.public_fivetran_parameters_dto import (
        PublicFivetranParametersDto,
    )
    from sifflet_sdk.client.model.public_looker_parameters_dto import (
        PublicLookerParametersDto,
    )
    from sifflet_sdk.client.model.public_mssql_parameters_dto import (
        PublicMssqlParametersDto,
    )
    from sifflet_sdk.client.model.public_mysql_parameters_dto import (
        PublicMysqlParametersDto,
    )
    from sifflet_sdk.client.model.public_oracle_parameters_dto import (
        PublicOracleParametersDto,
    )
    from sifflet_sdk.client.model.public_postgresql_parameters_dto import (
        PublicPostgresqlParametersDto,
    )
    from sifflet_sdk.client.model.public_power_bi_parameters_dto import (
        PublicPowerBiParametersDto,
    )
    from sifflet_sdk.client.model.public_quicksight_parameters_dto import (
        PublicQuicksightParametersDto,
    )
    from sifflet_sdk.client.model.public_redshift_parameters_dto import (
        PublicRedshiftParametersDto,
    )
    from sifflet_sdk.client.model.public_snowflake_parameters_dto import (
        PublicSnowflakeParametersDto,
    )
    from sifflet_sdk.client.model.public_synapse_parameters_dto import (
        PublicSynapseParametersDto,
    )
    from sifflet_sdk.client.model.public_tableau_parameters_dto import (
        PublicTableauParametersDto,
    )

    globals()["GitConnection"] = GitConnection
    globals()["PublicAirflowParametersDto"] = PublicAirflowParametersDto
    globals()["PublicAthenaParametersDto"] = PublicAthenaParametersDto
    globals()["PublicBigQueryParametersDto"] = PublicBigQueryParametersDto
    globals()["PublicDatabricksParametersDto"] = PublicDatabricksParametersDto
    globals()["PublicDbtCloudParametersDto"] = PublicDbtCloudParametersDto
    globals()["PublicDbtParametersDto"] = PublicDbtParametersDto
    globals()["PublicDeclarativeParametersDto"] = PublicDeclarativeParametersDto
    globals()["PublicFivetranParametersDto"] = PublicFivetranParametersDto
    globals()["PublicLookerParametersDto"] = PublicLookerParametersDto
    globals()["PublicMssqlParametersDto"] = PublicMssqlParametersDto
    globals()["PublicMysqlParametersDto"] = PublicMysqlParametersDto
    globals()["PublicOracleParametersDto"] = PublicOracleParametersDto
    globals()["PublicPostgresqlParametersDto"] = PublicPostgresqlParametersDto
    globals()["PublicPowerBiParametersDto"] = PublicPowerBiParametersDto
    globals()["PublicQuicksightParametersDto"] = PublicQuicksightParametersDto
    globals()["PublicRedshiftParametersDto"] = PublicRedshiftParametersDto
    globals()["PublicSnowflakeParametersDto"] = PublicSnowflakeParametersDto
    globals()["PublicSynapseParametersDto"] = PublicSynapseParametersDto
    globals()["PublicTableauParametersDto"] = PublicTableauParametersDto


class PublicCreateSourceDtoParameters(ModelComposed):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Attributes:
      allowed_values (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          with a capitalized key describing the allowed value and an allowed
          value. These dicts store the allowed enum values.
      attribute_map (dict): The key is attribute name
          and the value is json key in definition.
      discriminator_value_class_map (dict): A dict to go from the discriminator
          variable value to the discriminator class name.
      validations (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          that stores validations for max_length, min_length, max_items,
          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,
          inclusive_minimum, and regex.
      additional_properties_type (tuple): A tuple of classes accepted
          as additional properties values.
    """

    allowed_values = {
        ("type",): {
            "ATHENA": "ATHENA",
            "BIGQUERY": "BIGQUERY",
            "REDSHIFT": "REDSHIFT",
            "SNOWFLAKE": "SNOWFLAKE",
            "DATABRICKS": "DATABRICKS",
            "MSSQL": "MSSQL",
            "MYSQL": "MYSQL",
            "POSTGRESQL": "POSTGRESQL",
            "ORACLE": "ORACLE",
            "SYNAPSE": "SYNAPSE",
            "LOOKER": "LOOKER",
            "TABLEAU": "TABLEAU",
            "QUICKSIGHT": "QUICKSIGHT",
            "AIRFLOW": "AIRFLOW",
            "DBT": "DBT",
            "DBT_CLOUD": "DBT_CLOUD",
            "FIVETRAN": "FIVETRAN",
            "POWER_BI": "POWER_BI",
            "DECLARATIVE": "DECLARATIVE",
            "_UNKNOWN_": "_UNKNOWN_",
        },
        ("mysql_tls_version",): {
            "2": "TLS_V_1_2",
            "3": "TLS_V_1_3",
        },
    }

    validations = {
        ("worker_project_ids",): {},
    }

    @cached_property
    def additional_properties_type():
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded
        """
        lazy_import()
        return (
            bool,
            date,
            datetime,
            dict,
            float,
            int,
            list,
            str,
            none_type,
        )  # noqa: E501

    _nullable = False

    @cached_property
    def openapi_types():
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded

        Returns
            openapi_types (dict): The key is attribute name
                and the value is attribute type.
        """
        lazy_import()
        return {
            "type": (str,),  # noqa: E501
            "vpc_url": (str,),  # noqa: E501
            "billing_project_id": (str,),  # noqa: E501
            "worker_project_ids": ([str],),  # noqa: E501
            "job_definition_id": (str,),  # noqa: E501
            "site": (str,),  # noqa: E501
            "database": (str,),  # noqa: E501
            "datasource": (str,),  # noqa: E501
            "region": (str,),  # noqa: E501
            "role_arn": (str,),  # noqa: E501
            "s3_output_location": (str,),  # noqa: E501
            "workgroup": (str,),  # noqa: E501
            "dataset_id": (str,),  # noqa: E501
            "project_id": (str,),  # noqa: E501
            "catalog": (str,),  # noqa: E501
            "host": (str,),  # noqa: E501
            "http_path": (str,),  # noqa: E501
            "port": (int,),  # noqa: E501
            "schema": (str,),  # noqa: E501
            "account_id": (str,),  # noqa: E501
            "base_url": (str,),  # noqa: E501
            "project_name": (str,),  # noqa: E501
            "target": (str,),  # noqa: E501
            "git_connections": ([GitConnection],),  # noqa: E501
            "ssl": (bool,),  # noqa: E501
            "mysql_tls_version": (str,),  # noqa: E501
            "client_id": (str,),  # noqa: E501
            "tenant_id": (str,),  # noqa: E501
            "workspace_id": (str,),  # noqa: E501
            "aws_region": (str,),  # noqa: E501
            "account_identifier": (str,),  # noqa: E501
            "warehouse": (str,),  # noqa: E501
        }

    @cached_property
    def discriminator():
        lazy_import()
        val = {
            "AIRFLOW": PublicAirflowParametersDto,
            "ATHENA": PublicAthenaParametersDto,
            "BIGQUERY": PublicBigQueryParametersDto,
            "DATABRICKS": PublicDatabricksParametersDto,
            "DBT": PublicDbtParametersDto,
            "DBT_CLOUD": PublicDbtCloudParametersDto,
            "DECLARATIVE": PublicDeclarativeParametersDto,
            "FIVETRAN": PublicFivetranParametersDto,
            "LOOKER": PublicLookerParametersDto,
            "MSSQL": PublicMssqlParametersDto,
            "MYSQL": PublicMysqlParametersDto,
            "ORACLE": PublicOracleParametersDto,
            "POSTGRESQL": PublicPostgresqlParametersDto,
            "POWER_BI": PublicPowerBiParametersDto,
            "PublicAirflowParametersDto": PublicAirflowParametersDto,
            "PublicAthenaParametersDto": PublicAthenaParametersDto,
            "PublicBigQueryParametersDto": PublicBigQueryParametersDto,
            "PublicDatabricksParametersDto": PublicDatabricksParametersDto,
            "PublicDbtCloudParametersDto": PublicDbtCloudParametersDto,
            "PublicDbtParametersDto": PublicDbtParametersDto,
            "PublicDeclarativeParametersDto": PublicDeclarativeParametersDto,
            "PublicFivetranParametersDto": PublicFivetranParametersDto,
            "PublicLookerParametersDto": PublicLookerParametersDto,
            "PublicMssqlParametersDto": PublicMssqlParametersDto,
            "PublicMysqlParametersDto": PublicMysqlParametersDto,
            "PublicOracleParametersDto": PublicOracleParametersDto,
            "PublicPostgresqlParametersDto": PublicPostgresqlParametersDto,
            "PublicPowerBiParametersDto": PublicPowerBiParametersDto,
            "PublicQuicksightParametersDto": PublicQuicksightParametersDto,
            "PublicRedshiftParametersDto": PublicRedshiftParametersDto,
            "PublicSnowflakeParametersDto": PublicSnowflakeParametersDto,
            "PublicSynapseParametersDto": PublicSynapseParametersDto,
            "PublicTableauParametersDto": PublicTableauParametersDto,
            "QUICKSIGHT": PublicQuicksightParametersDto,
            "REDSHIFT": PublicRedshiftParametersDto,
            "SNOWFLAKE": PublicSnowflakeParametersDto,
            "SYNAPSE": PublicSynapseParametersDto,
            "TABLEAU": PublicTableauParametersDto,
        }
        if not val:
            return None
        return {"type": val}

    attribute_map = {
        "type": "type",  # noqa: E501
        "vpc_url": "vpcUrl",  # noqa: E501
        "billing_project_id": "billingProjectId",  # noqa: E501
        "worker_project_ids": "workerProjectIds",  # noqa: E501
        "job_definition_id": "jobDefinitionId",  # noqa: E501
        "site": "site",  # noqa: E501
        "database": "database",  # noqa: E501
        "datasource": "datasource",  # noqa: E501
        "region": "region",  # noqa: E501
        "role_arn": "roleArn",  # noqa: E501
        "s3_output_location": "s3OutputLocation",  # noqa: E501
        "workgroup": "workgroup",  # noqa: E501
        "dataset_id": "datasetId",  # noqa: E501
        "project_id": "projectId",  # noqa: E501
        "catalog": "catalog",  # noqa: E501
        "host": "host",  # noqa: E501
        "http_path": "httpPath",  # noqa: E501
        "port": "port",  # noqa: E501
        "schema": "schema",  # noqa: E501
        "account_id": "accountId",  # noqa: E501
        "base_url": "baseUrl",  # noqa: E501
        "project_name": "projectName",  # noqa: E501
        "target": "target",  # noqa: E501
        "git_connections": "gitConnections",  # noqa: E501
        "ssl": "ssl",  # noqa: E501
        "mysql_tls_version": "mysqlTlsVersion",  # noqa: E501
        "client_id": "clientId",  # noqa: E501
        "tenant_id": "tenantId",  # noqa: E501
        "workspace_id": "workspaceId",  # noqa: E501
        "aws_region": "awsRegion",  # noqa: E501
        "account_identifier": "accountIdentifier",  # noqa: E501
        "warehouse": "warehouse",  # noqa: E501
    }

    read_only_vars = {}

    @classmethod
    @convert_js_args_to_python_args
    def _from_openapi_data(cls, *args, **kwargs):  # noqa: E501
        """PublicCreateSourceDtoParameters - a model defined in OpenAPI

        Keyword Args:
            type (str):
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            vpc_url (str): Your VPC URL for Athena connection. [optional]  # noqa: E501
            billing_project_id (str): Your billing project ID. [optional]  # noqa: E501
            worker_project_ids ([str]): Comma separated list of project ids where your queries run. Optional if it's the same as the projectId parameter. [optional]  # noqa: E501
            job_definition_id (str): Your dbt Cloud job ID. [optional]  # noqa: E501
            site (str): Your Tableau Server site. Leave empty if your Tableau environment is using the Default Site.. [optional]  # noqa: E501
            database (str): Your database name. [optional]  # noqa: E501
            datasource (str): Your Athena data source name. [optional]  # noqa: E501
            region (str): Your Athena instance AWS region. [optional]  # noqa: E501
            role_arn (str): The ARN for your QuickSight role. [optional]  # noqa: E501
            s3_output_location (str): The S3 location where Athena query results are stored. [optional]  # noqa: E501
            workgroup (str): Your Athena workgroup name. [optional]  # noqa: E501
            dataset_id (str): Your BigQuery dataset ID. [optional]  # noqa: E501
            project_id (str): Your dbt Cloud project ID. [optional]  # noqa: E501
            catalog (str): Your Databricks catalog. [optional]  # noqa: E501
            host (str): Your Tableau Server hostname. [optional]  # noqa: E501
            http_path (str): Your Databricks HTTP path. [optional]  # noqa: E501
            port (int): Your Synapse server port. [optional]  # noqa: E501
            schema (str): Your schema name. [optional]  # noqa: E501
            account_id (str): Your AWS account ID. [optional]  # noqa: E501
            base_url (str): Your dbt Cloud base URL. [optional]  # noqa: E501
            project_name (str): Your dbt project name (the 'name' value in your dbt_project.yml file). [optional]  # noqa: E501
            target (str): Your dbt target name (the 'target' value in your profiles.yml file). [optional]  # noqa: E501
            git_connections ([GitConnection]): The LookML configuration. See https://docs.siffletdata.com/docs/looker. If you don't use LookML, use an empty list `[]`. [optional]  # noqa: E501
            ssl (bool): Whether to use SSL to connect to your Redshift server. [optional]  # noqa: E501
            mysql_tls_version (str): The TLS version to use to connect to your MySQL server. [optional]  # noqa: E501
            client_id (str): Your Azure AD client ID. [optional]  # noqa: E501
            tenant_id (str): Your Azure AD tenant ID. [optional]  # noqa: E501
            workspace_id (str): Your Power BI workspace ID. [optional]  # noqa: E501
            aws_region (str): Your AWS region. [optional]  # noqa: E501
            account_identifier (str): Your Snowflake account identifier. [optional]  # noqa: E501
            warehouse (str): Your Snowflake warehouse. [optional]  # noqa: E501
        """

        _check_type = kwargs.pop("_check_type", True)
        _spec_property_naming = kwargs.pop("_spec_property_naming", False)
        _path_to_item = kwargs.pop("_path_to_item", ())
        _configuration = kwargs.pop("_configuration", None)
        _visited_composed_classes = kwargs.pop("_visited_composed_classes", ())

        self = super(OpenApiModel, cls).__new__(cls)

        if args:
            for arg in args:
                if isinstance(arg, dict):
                    kwargs.update(arg)
                else:
                    raise ApiTypeError(
                        "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments."
                        % (
                            args,
                            self.__class__.__name__,
                        ),
                        path_to_item=_path_to_item,
                        valid_classes=(self.__class__,),
                    )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        constant_args = {
            "_check_type": _check_type,
            "_path_to_item": _path_to_item,
            "_spec_property_naming": _spec_property_naming,
            "_configuration": _configuration,
            "_visited_composed_classes": self._visited_composed_classes,
        }
        composed_info = validate_get_composed_info(constant_args, kwargs, self)
        self._composed_instances = composed_info[0]
        self._var_name_to_model_instances = composed_info[1]
        self._additional_properties_model_instances = composed_info[2]
        discarded_args = composed_info[3]

        for var_name, var_value in kwargs.items():
            if (
                var_name in discarded_args
                and self._configuration is not None
                and self._configuration.discard_unknown_keys
                and self._additional_properties_model_instances
            ):
                # discard variable.
                continue
            setattr(self, var_name, var_value)

        return self

    required_properties = set(
        [
            "_data_store",
            "_check_type",
            "_spec_property_naming",
            "_path_to_item",
            "_configuration",
            "_visited_composed_classes",
            "_composed_instances",
            "_var_name_to_model_instances",
            "_additional_properties_model_instances",
        ]
    )

    @convert_js_args_to_python_args
    def __init__(self, *args, **kwargs):  # noqa: E501
        """PublicCreateSourceDtoParameters - a model defined in OpenAPI

        Keyword Args:
            type (str):
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            vpc_url (str): Your VPC URL for Athena connection. [optional]  # noqa: E501
            billing_project_id (str): Your billing project ID. [optional]  # noqa: E501
            worker_project_ids ([str]): Comma separated list of project ids where your queries run. Optional if it's the same as the projectId parameter. [optional]  # noqa: E501
            job_definition_id (str): Your dbt Cloud job ID. [optional]  # noqa: E501
            site (str): Your Tableau Server site. Leave empty if your Tableau environment is using the Default Site.. [optional]  # noqa: E501
            database (str): Your database name. [optional]  # noqa: E501
            datasource (str): Your Athena data source name. [optional]  # noqa: E501
            region (str): Your Athena instance AWS region. [optional]  # noqa: E501
            role_arn (str): The ARN for your QuickSight role. [optional]  # noqa: E501
            s3_output_location (str): The S3 location where Athena query results are stored. [optional]  # noqa: E501
            workgroup (str): Your Athena workgroup name. [optional]  # noqa: E501
            dataset_id (str): Your BigQuery dataset ID. [optional]  # noqa: E501
            project_id (str): Your dbt Cloud project ID. [optional]  # noqa: E501
            catalog (str): Your Databricks catalog. [optional]  # noqa: E501
            host (str): Your Tableau Server hostname. [optional]  # noqa: E501
            http_path (str): Your Databricks HTTP path. [optional]  # noqa: E501
            port (int): Your Synapse server port. [optional]  # noqa: E501
            schema (str): Your schema name. [optional]  # noqa: E501
            account_id (str): Your AWS account ID. [optional]  # noqa: E501
            base_url (str): Your dbt Cloud base URL. [optional]  # noqa: E501
            project_name (str): Your dbt project name (the 'name' value in your dbt_project.yml file). [optional]  # noqa: E501
            target (str): Your dbt target name (the 'target' value in your profiles.yml file). [optional]  # noqa: E501
            git_connections ([GitConnection]): The LookML configuration. See https://docs.siffletdata.com/docs/looker. If you don't use LookML, use an empty list `[]`. [optional]  # noqa: E501
            ssl (bool): Whether to use SSL to connect to your Redshift server. [optional]  # noqa: E501
            mysql_tls_version (str): The TLS version to use to connect to your MySQL server. [optional]  # noqa: E501
            client_id (str): Your Azure AD client ID. [optional]  # noqa: E501
            tenant_id (str): Your Azure AD tenant ID. [optional]  # noqa: E501
            workspace_id (str): Your Power BI workspace ID. [optional]  # noqa: E501
            aws_region (str): Your AWS region. [optional]  # noqa: E501
            account_identifier (str): Your Snowflake account identifier. [optional]  # noqa: E501
            warehouse (str): Your Snowflake warehouse. [optional]  # noqa: E501
        """

        _check_type = kwargs.pop("_check_type", True)
        _spec_property_naming = kwargs.pop("_spec_property_naming", False)
        _path_to_item = kwargs.pop("_path_to_item", ())
        _configuration = kwargs.pop("_configuration", None)
        _visited_composed_classes = kwargs.pop("_visited_composed_classes", ())

        if args:
            for arg in args:
                if isinstance(arg, dict):
                    kwargs.update(arg)
                else:
                    raise ApiTypeError(
                        "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments."
                        % (
                            args,
                            self.__class__.__name__,
                        ),
                        path_to_item=_path_to_item,
                        valid_classes=(self.__class__,),
                    )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        constant_args = {
            "_check_type": _check_type,
            "_path_to_item": _path_to_item,
            "_spec_property_naming": _spec_property_naming,
            "_configuration": _configuration,
            "_visited_composed_classes": self._visited_composed_classes,
        }
        composed_info = validate_get_composed_info(constant_args, kwargs, self)
        self._composed_instances = composed_info[0]
        self._var_name_to_model_instances = composed_info[1]
        self._additional_properties_model_instances = composed_info[2]
        discarded_args = composed_info[3]

        for var_name, var_value in kwargs.items():
            if (
                var_name in discarded_args
                and self._configuration is not None
                and self._configuration.discard_unknown_keys
                and self._additional_properties_model_instances
            ):
                # discard variable.
                continue
            setattr(self, var_name, var_value)
            if var_name in self.read_only_vars:
                raise ApiAttributeError(
                    f"`{var_name}` is a read-only attribute. Use `from_openapi_data` to instantiate "
                    f"class with read only attributes."
                )

    @cached_property
    def _composed_schemas():
        # we need this here to make our import statements work
        # we must store _composed_schemas in here so the code is only run
        # when we invoke this method. If we kept this at the class
        # level we would get an error because the class level
        # code would be run when this module is imported, and these composed
        # classes don't exist yet because their module has not finished
        # loading
        lazy_import()
        return {
            "anyOf": [],
            "allOf": [],
            "oneOf": [
                PublicAirflowParametersDto,
                PublicAthenaParametersDto,
                PublicBigQueryParametersDto,
                PublicDatabricksParametersDto,
                PublicDbtCloudParametersDto,
                PublicDbtParametersDto,
                PublicDeclarativeParametersDto,
                PublicFivetranParametersDto,
                PublicLookerParametersDto,
                PublicMssqlParametersDto,
                PublicMysqlParametersDto,
                PublicOracleParametersDto,
                PublicPostgresqlParametersDto,
                PublicPowerBiParametersDto,
                PublicQuicksightParametersDto,
                PublicRedshiftParametersDto,
                PublicSnowflakeParametersDto,
                PublicSynapseParametersDto,
                PublicTableauParametersDto,
            ],
        }
