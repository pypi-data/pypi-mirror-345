#@package optimizer

# LICENSE HEADER MANAGED BY add-license-header
#
# SPDX-FileCopyrightText: Copyright 2024 German Cancer Research Center (DKFZ) and contributors.
# SPDX-License-Identifier: MIT
#

###
# default: :class:`~torch.optim.Adam`
#  - the Adam optimizer
#  - see `Adam <https://pytorch.org/docs/stable/generated/torch.optim.Adam.html>`_
_target_: torch.optim.Adam
###
# default: [ 0.9, 0.999 ]
#  - coefficients used for computing running averages of gradient and its square
betas: [ 0.9, 0.999 ]
###
# default: 0.0005
#  - the initial learning rate
lr: 0.0005
###
# default: 1e-08
#  - denominator summand for numerical stability
eps: 1e-08
###
# default: 0
#  - L2 penalty
weight_decay: 0
_convert_: "partial"
